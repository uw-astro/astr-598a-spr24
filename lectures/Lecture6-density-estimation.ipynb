{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### University of Washington: Machine Learning and Statistics \n",
    "\n",
    "# Lecture 6:  Density Estimation 1\n",
    "\n",
    "Andrew Connolly and Stephen Portillo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Resources for this notebook include:\n",
    "- [Textbook](https://press.princeton.edu/books/hardcover/9780691198309/statistics-data-mining-and-machine-learning-in-astronomy) Chapter 8. \n",
    "- [astroML website](https://www.astroml.org/index.html)\n",
    "\n",
    "This notebook is developed based on material from A. Connolly, Z. Ivezic, M. Juric, S. Portillo, G. Richards, B. Sipocz, J. VanderPlas, D. Hogg, Killian Weinberger and many others.\n",
    "\n",
    "The notebook and assoociated material are available from [github](https://github.com/uw-astro/astr-598a-win22).\n",
    "\n",
    "Make sure you are using the latest version of astroML\n",
    "\n",
    "> pip install --pre -U astroml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc'></a>\n",
    "\n",
    "## This notebook includes:\n",
    "\n",
    "[Introduction to Clustering ](#basics) \n",
    "\n",
    "[1-D hypothesis testing](#1Dht)\n",
    "\n",
    "[K-means clustering algorithm](#kmeans) \n",
    "\n",
    "[Kernel Density Estimation](#kde)\n",
    "\n",
    "[K-nearest neighbors](#knn) \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Clustering <a id='basics'></a>\n",
    "[Go to top](#toc)\n",
    "\n",
    "“Clustering” in astronomy refers to a number of different aspects of data analysis. Given a multivariate point data set, we can ask whether it displays any structure, that is, concentrations of points. Alternatively, when a density estimate is available we can search for “overdensities”. Another way to interpret clustering is to seek a partitioning or segmentation of data into smaller parts according to some criteria. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unsupervised vs. Supervised Classification  \n",
    "\n",
    "In density estimation, we estimate joint probability distributions from multivariate data sets to identify the inherent clustering. This is essentially **unsupervised classification**. Here “unsupervised” means that there is no prior information about the number and properties of clusters. In other words, this method is a search for unknown structure in your (multi-dimensional) dataset.\n",
    "\n",
    "If we have labels for some of these data points (e.g., an object is tall, short, red, or blue), we can develop a relationship between the label and the properties of a source. This is **supervised classification**. In other words, this method is finding objects in  your (multi-dimensional) dataset that \"look like\" objects in your training set. \n",
    "\n",
    "Classification, regression, and density estimation are all related. For example, the regression function $\\hat{y} = f(y|\\vec{x})$ is the best estimated value of $y$ given a value of $\\vec{x}$. In classification $y$ is categorical and $f(y|\\vec{x})$ is called the _discriminant function_\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1-D hypothesis testing <a id='1Dht'></a>\n",
    "[Go to top](#toc)\n",
    "\n",
    "How do we decide about the existance of a cluster? Let's start with\n",
    "the simplest but fundamental example: 1-D hypothesis testing.\n",
    "\n",
    "\n",
    "**Motivating question:** You just measured x = 3, with a negligible measurement error.\n",
    "\n",
    "You know that you could have drawn this value from one of two possible populations (e.g. stars and galaxies). One population can be described as N(0,2), and the other one as N(4,1). \n",
    "\n",
    "Which population is more likely, given your x?  \n",
    "\n",
    "Naive answer: 3 is closer to 4 (\"1 $\\sigma$ away\") than to 0\n",
    "(\"1.5 $\\sigma$ away\") so the second population is more likely.\n",
    "\n",
    "Let's see why this answer is wrong...\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the underlying distribution, h(x), is the sum of two populations\n",
    "\n",
    "$$h(x) = (1-a) h_B (x) + a h_S (x) $$\n",
    "\n",
    "with $a$ the normalization coefficient. Given ${x_i}$ we want to know $p_S(x_i)$ (which means $p_B(x_i) = 1 - p_S(x_i)$)\n",
    "\n",
    "![](figures/fig_classification_example_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can choose a classification boundary, $x_c$. From this we can defined the expected number of spurious sources (false positives or Type I errors)\n",
    "\n",
    "$$n_{spurious} =  N(1-a) \\int_{x_c}^{\\infty} h_B(x)dx $$\n",
    "\n",
    "and the number of missed (false negative or Type II errors) \n",
    "\n",
    "$$n_{missed} = N a \\int_{0}^{x_c} h_S(x)dx $$\n",
    "\n",
    "Number of sources will be \n",
    "$$n_{sources} = N a - n_{missed} + n_{spurious} $$\n",
    "\n",
    "The completeness of the sample (sometimes called the recall or sensitivity) is then \n",
    "\n",
    "$$\\eta = \\frac{N a - n_{missed} }{N a} = 1 - \\int_{0}^{x_c} h_S(x)dx $$\n",
    "\n",
    "and the contamination of the sample is \n",
    "\n",
    "$$\\epsilon = \\frac{n_{spurious}}{n_{source}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the decision boundary is the $x$ value at which each class is equally likely,\n",
    "\n",
    "$$\\pi_1 p_1(x) = \\pi_2 p_2(x) $$\n",
    "\n",
    "\n",
    "$\\pi_i$ is the prior on the object being in class $i$ (estimated from the relative numbers of sources in each class). The form of $h_S$ and $h_B$ and the priors are needed in deciding the classification threshold "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means clustering algorithm <a id='kmeans'></a>\n",
    "[Go to top](#toc)\n",
    "\n",
    "![](figures/kmeans.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question is: how do we find clusters or estimate density efficiently?\n",
    "\n",
    "<u> The _K-means_ algorithm </u>\n",
    "\n",
    "The first approach for finding clusters that is always taught is $K$-means (simple and works well)\n",
    "\n",
    "$K$-means partitions points into $K$ disjoint subsets ($C_k$) with each subset containing $N_k$\n",
    "points \n",
    "\n",
    "\n",
    "It minimizes the objective/cost/likelihood function,\n",
    "$\\sum_{k=1}^K \\sum_{i \\in C_k} || x_i - \\mu_k ||^2$\n",
    "\n",
    "$\\mu_k = \\frac{1}{N_k} \\sum_{i \\in C_k} x_i$ is the mean of the\n",
    "points in set $C_k$\n",
    "\n",
    "\n",
    "_Procedure:_\n",
    "\n",
    "1. define the number of clusters $K$\n",
    "2. choose the centroid, $\\mu_k$, of each of the $K$ clusters\n",
    "3. assign each point to the cluster that it is closest to\n",
    "4. update the centroid of each cluster by recomputing $\\mu_k$ according to the new assignments.\n",
    "5. goto (3) until there are no new assignments.\n",
    "\n",
    "Global optima are not guaranteed but the process never increases the sum-of-squares error.\n",
    "\n",
    "Typically we run multiple times with different starting values for the\n",
    "centroids of $C_k$.\n",
    "\n",
    "We will start with looking at the density of stars as a function of metalicity and use scikit-learns preprocessing. We use the StandardScaler function to normalize each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.patches import Ellipse\n",
    "from scipy.stats import norm\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from astroML.datasets import fetch_sdss_sspp\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Get data\n",
    "data = fetch_sdss_sspp(cleaned=True)\n",
    "X = np.vstack([data['FeH'], data['alphFe']]).T\n",
    "\n",
    "# truncate dataset for speed\n",
    "X = X[::5]\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Compute a 2D histogram  of the input\n",
    "# Fe vs H\n",
    "#O, Ne, Mg, Si, S, Ar, Ca, and Ti vs Fe\n",
    "H, FeH_bins, alphFe_bins = np.histogram2d(data['FeH'], data['alphFe'], 50)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Compute the KMeans clustering\n",
    "n_clusters = 2\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "clf = KMeans(n_clusters)\n",
    "clf.fit(scaler.fit_transform(X.astype(\"float\")))\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Visualize the results\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "ax = fig.add_subplot()\n",
    "\n",
    "# plot density\n",
    "ax = plt.axes()\n",
    "ax.imshow(H.T, origin='lower', interpolation='nearest', aspect='auto',\n",
    "          extent=[FeH_bins[0], FeH_bins[-1],\n",
    "                  alphFe_bins[0], alphFe_bins[-1]],\n",
    "          cmap=plt.cm.binary)\n",
    "\n",
    "# plot cluster centers\n",
    "cluster_centers = scaler.inverse_transform(clf.cluster_centers_)\n",
    "ax.scatter(cluster_centers[:, 0], cluster_centers[:, 1],\n",
    "           s=40, c='w', edgecolors='k')\n",
    "\n",
    "# plot cluster boundaries\n",
    "FeH_centers = 0.5 * (FeH_bins[1:] + FeH_bins[:-1])\n",
    "alphFe_centers = 0.5 * (alphFe_bins[1:] + alphFe_bins[:-1])\n",
    "\n",
    "Xgrid = np.meshgrid(FeH_centers, alphFe_centers)\n",
    "Xgrid = np.array(Xgrid).reshape((2, 50 * 50)).T\n",
    "\n",
    "H = clf.predict(scaler.transform(Xgrid)).reshape((50, 50))\n",
    "\n",
    "for i in range(n_clusters):\n",
    "    Hcp = H.copy()\n",
    "    flag = (Hcp == i)\n",
    "    Hcp[flag] = 1\n",
    "    Hcp[~flag] = 0\n",
    "\n",
    "    ax.contour(FeH_centers, alphFe_centers, Hcp, [-0.5, 0.5],\n",
    "               linewidths=2, colors='k')\n",
    "\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(0.3))\n",
    "ax.set_xlim(-1.101, 0.101)\n",
    "ax.set_ylim(alphFe_bins[0], alphFe_bins[-1])\n",
    "\n",
    "ax.set_xlabel(r'$\\rm [Fe/H]$')\n",
    "ax.set_ylabel(r'$\\rm [\\alpha/Fe]$')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***How do you choose the number of clusters?***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Density Estimation <a id='kde'></a>\n",
    "\n",
    "[Go to top](#toc)\n",
    "\n",
    "\n",
    "$N(x) = \\frac{1}{Nh^D} \\sum_{i=1}^N  K\\left( \\frac{d(x,x_i)}{h} \\right),$\n",
    "\n",
    "K: kernel (defined by the bandwidth h) is any smooth function which is positive at all values\n",
    "\n",
    "Too narrow a kernel, too spiky the results (high variance)\n",
    "\n",
    "Too broad a kernel, too smooth or washed out the results (bias)\n",
    "\n",
    "_Common kernels_\n",
    "\n",
    "Squard exponential (Normal): $ K(u) = \\frac{1}{(2\\pi)^{D/2}} e^{- u^2 / 2}$ D: dimension\n",
    "\n",
    "Tophat: $ K(u) = \\left\\{\n",
    "  \\begin{array}{ll}\n",
    "    \\frac{1}{V_D(r)} & {\\rm if}\\ u \\le r,\\\\\n",
    "    0                & {\\rm if}\\ u > r,\n",
    "  \\end{array}\n",
    "  \\right.$\n",
    "  \n",
    "Exponential: $  K(u) = \\frac{1}{D!\\, V_D(r)}e^{-|u|}$ \n",
    "\n",
    "with $V_D(r)$ the volume of a hypersphere radius $r$;  $V_D(r) = \\frac{2r^D\\pi^{D/2}}{D\\  \\Gamma(D/2)}$\n",
    "\n",
    "<img src=\"figures/funcs.png\">\n",
    "\n",
    "Perhaps surprisingly the primary feature is the bandwidth of these distributions not the exact shape. Choosing the bandwidth is usually done through cross-validation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate this, the plot projects galaxies in SDSS \"Great Wall\" as scatted points by their spatial locations onto the equatorial plane (declination ~ $0^o$). The graph below shows the location of each point, but it is hard to get \"clustered information\" from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import LogNorm\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from astroML.datasets import fetch_great_wall\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# This function adjusts matplotlib settings for a uniform feel in the textbook.\n",
    "# Note that with usetex=True, fonts are rendered with LaTeX.  This may\n",
    "# result in an error if LaTeX is not installed on your system.  In that case,\n",
    "# you can set usetex to False.\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Fetch the great wall data\n",
    "X = fetch_great_wall()\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Create  the grid on which to evaluate the results\n",
    "Nx = 50\n",
    "Ny = 125\n",
    "xmin, xmax = (-375, -175)\n",
    "ymin, ymax = (-300, 200)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Evaluate for several models\n",
    "Xgrid = np.vstack(map(np.ravel, np.meshgrid(np.linspace(xmin, xmax, Nx),\n",
    "                                            np.linspace(ymin, ymax, Ny)))).T\n",
    "\n",
    "kernels = ['gaussian']\n",
    "dens = []\n",
    "\n",
    "bandwidth=5\n",
    "kde = KernelDensity(bandwidth=bandwidth, kernel='gaussian')\n",
    "log_dens = kde.fit(X).score_samples(Xgrid)\n",
    "dens = X.shape[0] * np.exp(log_dens).reshape((Ny, Nx))\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Plot the results\n",
    "fig = plt.figure(figsize=(15, 8))\n",
    "fig.subplots_adjust(left=0.12, right=0.95, bottom=0.2, top=0.9,\n",
    "                    hspace=0.01, wspace=0.01)\n",
    "\n",
    "# First plot: scatter the points\n",
    "ax1 = plt.subplot(221, aspect='equal')\n",
    "ax1.scatter(X[:, 1], X[:, 0], s=1, lw=0, c='k')\n",
    "ax1.text(0.95, 0.9, \"input\", ha='right', va='top',\n",
    "         transform=ax1.transAxes,\n",
    "         bbox=dict(boxstyle='round', ec='k', fc='w'))\n",
    "\n",
    "# Second plot: gaussian kernel\n",
    "ax2 = plt.subplot(222, aspect='equal')\n",
    "ax2.imshow(dens.T, origin='lower', norm=LogNorm(),\n",
    "           extent=(ymin, ymax, xmin, xmax), cmap=plt.cm.binary)\n",
    "ax2.text(0.95, 0.9, \"Gaussian h={}\".format(bandwidth), ha='right', va='top',\n",
    "         transform=ax2.transAxes,\n",
    "         bbox=dict(boxstyle='round', ec='k', fc='w'))\n",
    "\n",
    "for ax in [ax1, ax2]:\n",
    "    ax.set_xlim(ymin, ymax - 0.01)\n",
    "    ax.set_ylim(xmin, xmax)\n",
    "\n",
    "for ax in [ax1, ax2]:\n",
    "    ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "    ax.set_xlabel('$y$ (Mpc)')\n",
    "\n",
    "for ax in [ax2]:\n",
    "    ax.yaxis.set_major_formatter(plt.NullFormatter())\n",
    "\n",
    "for ax in [ax1]:\n",
    "    ax.set_ylabel('$x$ (Mpc)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise: Use Kernel Density Estimation with any kernel you choose on the color-magnitude diagrams (CMDs) of the two data sets Field A and Field B. Plot the density for each CMD in each panel (i.e., $g-r$ on the x axis and $g$ on the y axis) - a Hess diagrams.\n",
    "\n",
    "Experiment with different kernel bandwidths, plotting one that visually seems \"best\" (i.e., a good balance of bias vs. variance) for each kernel.\n",
    "\n",
    "Don't forget to change the figure size so that individual panels have aspect ratios closer to what is common for color-magnitude diagrams (i.e., x:y ~ 4:6 or so).\n",
    "\n",
    "Subtract the \"best\" density for Field B from A to see if there are structures present in the CMD. What are they?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hess diagrams with SDSS data\n",
    "import pandas as pd\n",
    "fieldA = pd.read_csv('data/fieldA.csv')\n",
    "fieldB = pd.read_csv('data/fieldB.csv')\n",
    "\n",
    "# Add a column for color\n",
    "fieldA['g-r'] = fieldA.g - fieldA.r\n",
    "fieldB['g-r'] = fieldB.g - fieldB.r\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.add_subplot(121, aspect='equal')\n",
    "ax.scatter(fieldA['g-r'],fieldA['g'], s=1, alpha=0.5)\n",
    "ax.set_xlim(-0.5, 2)\n",
    "ax.set_ylim(22,11)\n",
    "ax.set_xlabel('g-r')\n",
    "ax.set_ylabel('g')\n",
    "\n",
    "ax = fig.add_subplot(122, aspect='equal')\n",
    "ax.scatter(fieldB['g-r'],fieldB['g'], s=1, alpha=0.5)\n",
    "ax.set_xlim(-0.5, 2)\n",
    "ax.set_ylim(22,11)\n",
    "ax.set_xlabel('g-r')\n",
    "ax.set_ylabel('g')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nearest neighbor estimation <a id='knn'></a>\n",
    "\n",
    "[Go to top](#toc)\n",
    "\n",
    "Simple (simplest?) density estimator heavily used in astrophysics (cluster detection, large scale structure measures), originally proposed by [Dressler et al. 1980](https://ui.adsabs.harvard.edu/abs/1980ApJ...236..351D/abstract) . \n",
    "\n",
    "For each point we find the distance to the $K$th-nearest neighbor, $d_K$. **Note: we are not choosing clusters here** In this method, the implied point density at an arbitrary position x is estimated as\n",
    "\n",
    "$$\\hat{f_K}(x) = \\frac{K}{V_D(d_K)}$$\n",
    "\n",
    "where $V_D$ is evaluated volume, and D is the problem dimensionality.  \n",
    "  \n",
    "By taking the assumption that the underlying density field is locally constant, we can further simplify this method as\n",
    "\n",
    "$$\\hat{f_K}(x) = \\frac{C}{d_K^D}$$\n",
    "\n",
    "where C is a scaling factor evaluated by requiring that the sum of the product of $\\hat{f_K}(x)$ and\n",
    "pixel volume is equal to the total number of data points.\n",
    "\n",
    "The error on $\\hat{f}_K(x)$ is \n",
    "\n",
    "$$\\sigma_f = K^{1/2}/V_D (d_K)$$\n",
    "\n",
    "The fractional (relative) error is \n",
    "\n",
    "$$\\sigma_f/\\hat{f} = 1/K^{1/2}$$.\n",
    "\n",
    "We can see that the\n",
    "*   fractional accuracy increases with $K$ at  expense of the spatial resolution (bias-variance trade-off)\n",
    "*   effective resolution scales with $K^{1/D}$\n",
    "\n",
    "The method can be improved by considering distances to _all_ $K$ nearest neighbors \n",
    "\n",
    "$$\\hat{f}_K(x) = {C \\over \\sum_{i=1}^K d_i^D}$$\n",
    "\n",
    "The normalization when  computing local density without regard to overall mean density is\n",
    "\n",
    "$$C  =  \\frac{K\\, (K + 1)}{2 V_D(r)}$$\n",
    "\n",
    "In this method, we can change parameter k to get different estimation result. k should be at least 5 because the estimator is biased and has a large variance for smaller k; see [Casertano, S. and Hut, P.](https://ui.adsabs.harvard.edu/abs/1985ApJ...298...80C/abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KernelDensity\n",
    "from astroML.density_estimation import KNeighborsDensity\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Create  the grid on which to evaluate the results\n",
    "Nx = 50\n",
    "Ny = 125\n",
    "xmin, xmax = (-375, -175)\n",
    "ymin, ymax = (-300, 200)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Evaluate for several models\n",
    "Xgrid = np.vstack(map(np.ravel, np.meshgrid(np.linspace(xmin, xmax, Nx),\n",
    "                                            np.linspace(ymin, ymax, Ny)))).T\n",
    "\n",
    "kde = KernelDensity(kernel='gaussian', bandwidth=5)\n",
    "log_pdf_kde = kde.fit(X).score_samples(Xgrid).reshape((Ny, Nx))\n",
    "dens_KDE = np.exp(log_pdf_kde)\n",
    "\n",
    "knn5 = KNeighborsDensity('bayesian', 5)\n",
    "dens_k5 = knn5.fit(X).eval(Xgrid).reshape((Ny, Nx))\n",
    "\n",
    "knn40 = KNeighborsDensity('bayesian', 40)\n",
    "dens_k40 = knn40.fit(X).eval(Xgrid).reshape((Ny, Nx))\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Plot the results\n",
    "fig = plt.figure(figsize=(9, 4.0))\n",
    "fig.subplots_adjust(left=0.1, right=0.95, bottom=0.14, top=0.9,\n",
    "                    hspace=0.01, wspace=0.01)\n",
    "\n",
    "# First plot: scatter the points\n",
    "ax1 = plt.subplot(221, aspect='equal')\n",
    "ax1.scatter(X[:, 1], X[:, 0], s=1, lw=0, c='k')\n",
    "ax1.text(0.98, 0.95, \"input\", ha='right', va='top',\n",
    "         transform=ax1.transAxes, fontsize=12,\n",
    "         bbox=dict(boxstyle='round', ec='k', fc='w'))\n",
    "\n",
    "# Second plot: KDE\n",
    "ax2 = plt.subplot(222, aspect='equal')\n",
    "ax2.imshow(dens_KDE.T, origin='lower', norm=LogNorm(),\n",
    "           extent=(ymin, ymax, xmin, xmax), cmap=plt.cm.binary)\n",
    "ax2.text(0.98, 0.95, \"KDE: gaussian $(h=5)$\", ha='right', va='top',\n",
    "         transform=ax2.transAxes, fontsize=12,\n",
    "         bbox=dict(boxstyle='round', ec='k', fc='w'))\n",
    "\n",
    "# Third plot: KNN, k=5\n",
    "ax3 = plt.subplot(223, aspect='equal')\n",
    "ax3.imshow(dens_k5.T, origin='lower', norm=LogNorm(),\n",
    "           extent=(ymin, ymax, xmin, xmax), cmap=plt.cm.binary)\n",
    "ax3.text(0.98, 0.95, \"KNN $(k=5)$\", ha='right', va='top',\n",
    "         transform=ax3.transAxes, fontsize=12,\n",
    "         bbox=dict(boxstyle='round', ec='k', fc='w'))\n",
    "\n",
    "# Fourth plot: KNN, k=40\n",
    "ax4 = plt.subplot(224, aspect='equal')\n",
    "ax4.imshow(dens_k40.T, origin='lower', norm=LogNorm(),\n",
    "           extent=(ymin, ymax, xmin, xmax), cmap=plt.cm.binary)\n",
    "ax4.text(0.98, 0.95, \"KNN $(k=40)$\", ha='right', va='top',\n",
    "         transform=ax4.transAxes, fontsize=12,\n",
    "         bbox=dict(boxstyle='round', ec='k', fc='w'))\n",
    "\n",
    "for ax in [ax1, ax2, ax3, ax4]:\n",
    "    ax.set_xlim(ymin, ymax - 0.01)\n",
    "    ax.set_ylim(xmin, xmax)\n",
    "\n",
    "for ax in [ax1, ax2]:\n",
    "    ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "\n",
    "for ax in [ax3, ax4]:\n",
    "    ax.set_xlabel('$y$ (Mpc)')\n",
    "\n",
    "for ax in [ax2, ax4]:\n",
    "    ax.yaxis.set_major_formatter(plt.NullFormatter())\n",
    "\n",
    "for ax in [ax1, ax3]:\n",
    "    ax.set_ylabel('$x$ (Mpc)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

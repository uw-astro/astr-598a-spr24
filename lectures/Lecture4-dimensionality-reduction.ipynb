{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### University of Washington: Machine Learning and Statistics \n",
    "\n",
    "# Lecture 4: Dimensionality Reduction\n",
    " \n",
    "Andrew Connolly and Stephen Portillo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Resources for this notebook include:\n",
    "- [Textbook](https://press.princeton.edu/books/hardcover/9780691198309/statistics-data-mining-and-machine-learning-in-astronomy) Chapter 8. \n",
    "- [astroML website](https://www.astroml.org/index.html)\n",
    "\n",
    "This notebook is developed based on material from A. Connolly, Z. Ivezic, M. Juric, S. Portillo, G. Richards, B. Sipocz, J. VanderPlas, D. Hogg, and many others.\n",
    "\n",
    "The notebook and assoociated material are available from [github](https://github.com/uw-astro/astr-598a-win22).\n",
    "\n",
    "xMake sure you are using the latest version of astroML\n",
    "> pip install --pre -U astroml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc'></a>\n",
    "\n",
    "## This notebook includes:\n",
    "\n",
    "\n",
    "[The curse of dimensionality](#curse) \n",
    "\n",
    "[Principal Component Analysis](#PCA)\n",
    "\n",
    "[Gappy PCA](#gappy)\n",
    " \n",
    "[Comparing PCA, NMF and ICA](#all3)\n",
    "\n",
    "[Manifold Learning](#LLE) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The curse of dimensionality <a id='curse'></a>\n",
    "[Go to top](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting and overfitting get worse with ''curse of dimensionality'' Bellman 1961\n",
    "\n",
    "Think about a hypersphere. Its volume is  given by\n",
    "\n",
    "\n",
    "$$  V_D(r) = \\frac{2r^D\\pi^{D/2}}{D\\  \\Gamma(D/2)}$$\n",
    "\n",
    "where $\\Gamma(z)$ is the complete gamma function, $D$ is the dimension, and $r$ the radius of the sphere.\n",
    "\n",
    "\n",
    "If you populated a hypercube of size $2r$ how much data would be enclosed by the hypersphere\n",
    "- as $D$ increases the fractional volume enclosed by the hypersphere goes to 0! \n",
    "\n",
    "For example: the SDSS comprises a sample of 357 million sources. \n",
    "- each source has 448 measured attributes\n",
    "- selecting just 30 (e.g., magnitude, size..) and normalizing the data range $-1$ to $1$\n",
    "\n",
    "probability of having one of the 357 million sources reside within a unit hypersphere 1 in 1.4$\\times 10^5$.\n",
    "\n",
    "### Solution: dimensionality reduction!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Increasing interest in dimensionality reduction\n",
    "<img width=\"500\" src=\"figures/PCA.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis <a id='PCA'></a>\n",
    "[Go to top](#toc)\n",
    "\n",
    "\n",
    "![An example: a bivariate gaussian distribution](figures/pca-scatter.png)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Points are  correlated along a particular direction which doesn't align with the initial choice of axes. \n",
    "* we should rotate our axes to align with this correlation. \n",
    "* rotation preserves the relative ordering of data\n",
    "\n",
    "Choose  rotation to maximize the ability to discriminate between the data points\n",
    "*   first axis, or <u>principal component</u>, is direction of maximal variance\n",
    "*   second principal component is orthogonal to the first component and maximizes the residual variance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Singular value decomposition (SVD)\n",
    "Common approach is eigenvalue decomposition of the covariance or correlation matrix,\n",
    "or singular value decomposition (SVD) of the data matrix\n",
    "\n",
    "$$U \\Sigma V^T = \\frac{1}{\\sqrt{N - 1}} X$$\n",
    "\n",
    "columns of $U$ are  _left-singular vectors_\n",
    "\n",
    "columns of $V$ are the _right-singular vectors_\n",
    "\n",
    "The columns of $U$ and $V$ form orthonormal bases ($U^TU = V^TV = I$)\n",
    "\n",
    "Covariance matrix is\n",
    "\n",
    "$$\n",
    "  C_X = \\left[\\frac{1}{\\sqrt{N - 1}}X\\right]^T \\left[\\frac{1}{\\sqrt{N - 1}}X\\right]\\nonumber\\\\\n",
    "      = V \\Sigma U^T U \\Sigma V^T\\nonumber\\\\\n",
    "      = V \\Sigma^2 V^T.\n",
    "$$\n",
    "\n",
    "right singular vectors $V$ are the principal components so principal from the SVD of $X$ dont need $C_X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Application of PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the PCA components we use `PCA` from `scikit-learn`. \n",
    "\n",
    "- Center data by subtracting the mean of each dimension\n",
    "-  For heterogeneous data (e.g., galaxy shape and flux) divide by variance (whitening). why?\n",
    "- For spectra or images normalize each row so integrated flux of each object is one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our astronomy use case, we are using the SDSS spectroscopic dataset. \n",
    "The SDSS spectra come from galaxies at a range of redshifts,\n",
    "and have sections of unreliable or missing data due to sky absorption, cosmic rays, bad detector pixels,\n",
    "or other effects. AstroML provides a set of spectra which have been moved to rest frame, corrected\n",
    "for masking using an iterative PCA reconstruction technique, and resampled to 1000 common wavelength bins. \n",
    "\n",
    "The spectra can be downloaded using `fetch_sdss_corrected_spectra()`. In the following example we plot\n",
    "15 of these spectra:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from astroML.datasets import sdss_corrected_spectra\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Download data\n",
    "data = sdss_corrected_spectra.fetch_sdss_corrected_spectra()\n",
    "spectra_raw = data['spectra']\n",
    "spectra = sdss_corrected_spectra.reconstruct_spectra(data)\n",
    "wavelengths = sdss_corrected_spectra.compute_wavelengths(data)\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# Compute PCA: returns mean + PCA vectors\n",
    "def compute_PCA(n_components=20):\n",
    "    nrows = 5000\n",
    "    ind = np.random.randint(spectra.shape[0], size=nrows)\n",
    "    spec_mean = spectra[ind].mean(0)\n",
    "\n",
    "    # PCA\n",
    "    pca = PCA(n_components - 1)\n",
    "    pca.fit(spectra[ind])\n",
    "    pca_comp = np.vstack([spec_mean,\n",
    "                          pca.components_])\n",
    "    evals = pca.explained_variance_ratio_\n",
    "\n",
    "    return pca_comp, evals, ind\n",
    "\n",
    "n_components = 5\n",
    "decompositions, evals, index = compute_PCA(n_components)\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# Plot the results\n",
    "fig = plt.figure(figsize=(11, 20))\n",
    "\n",
    "fig.subplots_adjust(left=0.05, right=0.95, wspace=0.05,\n",
    "                    bottom=0.1, top=0.95, hspace=0.05)\n",
    "\n",
    "titles = 'PCA components'\n",
    "\n",
    "for j in range(5):\n",
    "    ax = fig.add_subplot(n_components, 2, 2*j+2)\n",
    "\n",
    "    ax.yaxis.set_major_formatter(plt.NullFormatter())\n",
    "    ax.xaxis.set_major_locator(plt.MultipleLocator(1000))\n",
    "    if j < n_components - 1:\n",
    "        ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "    else:\n",
    "        ax.set_xlabel(r'wavelength ${\\rm (\\AA)}$')\n",
    "    ax.plot(wavelengths, decompositions[j], '-k', lw=1)\n",
    "\n",
    "    # plot zero line\n",
    "    xlim = [3000, 7999]\n",
    "    ax.plot(xlim, [0, 0], '-', c='gray', lw=1)\n",
    "    ax.set_xlim(xlim)\n",
    "\n",
    "    # adjust y limits\n",
    "    ylim = plt.ylim()\n",
    "    dy = 0.05 * (ylim[1] - ylim[0])    \n",
    "    ax.set_ylim(ylim[0] - dy, ylim[1] + 4 * dy)\n",
    "\n",
    "\n",
    "    ax2 = fig.add_subplot(n_components, 2, 2*j+1)\n",
    "    ax2.yaxis.set_major_formatter(plt.NullFormatter())\n",
    "    ax2.xaxis.set_major_locator(plt.MultipleLocator(1000))\n",
    "    if j < n_components - 1:\n",
    "        ax2.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "    else:\n",
    "        ax2.set_xlabel(r'wavelength ${\\rm (\\AA)}$')\n",
    "    ax2.plot(wavelengths, spectra[j], '-k', lw=1)\n",
    "    \n",
    "    # plot zero line\n",
    "    ax2.plot(xlim, [0, 0], '-', c='gray', lw=1)\n",
    "    ax2.set_xlim(xlim)\n",
    "\n",
    "    if j == 0:\n",
    "        ax.set_title(titles, fontsize='medium')\n",
    "\n",
    "    if j == 0:\n",
    "        label = 'mean'\n",
    "    else:\n",
    "        label = 'component %i' % j\n",
    "\n",
    "    # adjust y limits\n",
    "    ylim = plt.ylim()\n",
    "    dy = 0.05 * (ylim[1] - ylim[0])    \n",
    "    ax2.set_ylim(ylim[0] - dy, ylim[1] + 4 * dy)\n",
    "\n",
    "\n",
    "    ax.text(0.02, 0.95, label, transform=ax.transAxes,\n",
    "            ha='left', va='top', bbox=dict(ec='w', fc='w'),\n",
    "            fontsize='small')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Plot the eigenvalues and what they meam in terms of explained variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpreting the PCA\n",
    "\n",
    "Reconstruction of spectrum, ${x}(k)$, from the\n",
    "eigenvectors, ${e}_i(k)$ \n",
    "\n",
    ">$ \\begin{equation}\n",
    "  {x}_i(k) = {\\mu}(k) + \\sum_j^R \\theta_{ij} {e}_j(k),\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "Truncating this expansion (i.e., $r<R$)\n",
    "\n",
    ">$\\begin{equation}\n",
    "{x}_i(k) = {\\mu}(k) + \\sum_i^{r<R} \\theta_i {e}_i(k),\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "- eigenvectors ordered by their associated eigenvalues \n",
    "- eigenvalues reflect variance  within each eigenvector (sum of the eigenvalues is total variance of the system).\n",
    "- project each spectrum onto these first few eigenspectra is a compression of the data \n",
    "\n",
    "This is the sense in which PCA gives for dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Because the spectra have been reconstructed from masked values, this\n",
    "#  is not exactly correct in this case: we'll use the values computed\n",
    "#  in the file compute_sdss_pca.py\n",
    "evals = data['evals'] ** 2\n",
    "evals_cs = evals.cumsum()\n",
    "evals_cs /= evals_cs[-1]\n",
    "evecs = data['evecs']\n",
    "spec_mean = spectra.mean(0)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Find the coefficients of a particular spectrum\n",
    "spec = spectra[1]\n",
    "coeff = np.dot(evecs, spec - spec_mean)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Plot the sequence of reconstructions\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "fig.subplots_adjust(hspace=0, top=0.95, bottom=0.1, left=0.12, right=0.93)\n",
    "\n",
    "for i, n in enumerate([0, 4, 8, 20]):\n",
    "    ax = fig.add_subplot(411 + i)\n",
    "    ax.plot(wavelengths, spec, '-', c='gray')\n",
    "    ax.plot(wavelengths, spec_mean + np.dot(coeff[:n], evecs[:n]), '-k')\n",
    "\n",
    "    if i < 3:\n",
    "        ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "\n",
    "    ax.set_ylim(-2, 21)\n",
    "    ax.set_ylabel('flux')\n",
    "\n",
    "    if n == 0:\n",
    "        text = \"mean\"\n",
    "    elif n == 1:\n",
    "        text = \"mean + 1 component\\n\"\n",
    "        text += r\"$(\\sigma^2_{tot} = %.2f)$\" % evals_cs[n - 1]\n",
    "    else:\n",
    "        text = \"mean + %i components\\n\" % n\n",
    "        text += r\"$(\\sigma^2_{tot} = %.2f)$\" % evals_cs[n - 1]\n",
    "\n",
    "    ax.text(0.02, 0.93, text, ha='left', va='top', transform=ax.transAxes)\n",
    "\n",
    "fig.axes[-1].set_xlabel(r'${\\rm wavelength\\ (\\AA)}$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA with missing data <a id='gappy'></a>\n",
    "[Go to top](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observed spectrum, $x^o$ is the true spectrum, ${x}$ plus a wavelength-dependent weight, ${w}$.  \n",
    "\n",
    "Weight is zero where data are missing and $1/{\\sigma}^2$ for rest\n",
    "\n",
    "Minimizing the quadratic deviation between  ${x}^o$ truncated reconstruction, $\\sum_i \\theta_i {e}_i$\n",
    "and solving for $\\theta_i$ gives\n",
    "\n",
    "$$\n",
    "\t\\sum_k \\theta_i {w}(k) {e}_i(k) {e}_j(k) =\n",
    "\t\\sum_k {w}(k) {x}^o(k) {e}_j(k),\n",
    "$$\n",
    "\n",
    "If $M_{ij} = \\sum_k {w}(k) {e}_i(k) {e}_j(k)$ and $F_i = \\sum_k {w}(k) {x}^o(k) {e}_i(k)$ then \n",
    "\n",
    "$$\n",
    "\t\\theta_i = \\sum_j M_{ij}^{-1} F_{j},\n",
    "$$\n",
    "\n",
    "- $F_j$ are  coefficients derived from  gappy data\n",
    "- $M_{ij}^{-1}$ shows how correlated  eigenvectors are over the missing regions.\n",
    "\n",
    "An estimate of the uncertainty on the\n",
    "reconstruction coefficients is given by\n",
    "\n",
    "$$\n",
    "%Cov(\\theta_i,\\theta_j) = \\frac{1}{N}M_{ij}^{-1}\n",
    "{\\rm Cov}(\\theta_i,\\theta_j) = M_{ij}^{-1}.\n",
    "$$\n",
    "\n",
    "Accuracy of this reconstruction will depend on the distribution of\n",
    "the gaps within the data vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The principal component vectors defined for the SDSS spectra can be used to interpolate across or reconstruct\n",
    "missing data. Examples of three masked spectral regions are shown comparing the reconstruction of the input \n",
    "spectrum (black line) using the mean and the first ten eigenspectra (blue line) The gray bands represent the \n",
    "masked region of the spectrum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectra = sdss_corrected_spectra.reconstruct_spectra(data)\n",
    "wavelengths = sdss_corrected_spectra.compute_wavelengths(data)\n",
    "\n",
    "evecs = data['evecs']\n",
    "mu = data['mu']\n",
    "norms = data['norms']\n",
    "mask = data['mask']\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# plot the results\n",
    "i_plot = ((wavelengths > 5750) & (wavelengths < 6350))\n",
    "wavelength = wavelengths[i_plot]\n",
    "\n",
    "specnums = [20, 8, 9]\n",
    "subplots = [311, 312, 313]\n",
    "\n",
    "fig = plt.figure(figsize=(10, 12.5))\n",
    "fig.subplots_adjust(left=0.09, bottom=0.08, hspace=0, right=0.92, top=0.95)\n",
    "\n",
    "for subplot, i in zip(subplots, specnums):\n",
    "    ax = fig.add_subplot(subplot)\n",
    "\n",
    "    # compute eigen-coefficients\n",
    "    spectra_i_centered = spectra[i] / norms[i] - mu\n",
    "    coeffs = np.dot(spectra_i_centered, evecs.T)\n",
    "\n",
    "    # plot the raw  spectrum\n",
    "    spectra_i = spectra[i]\n",
    "    ax.plot(wavelength, spectra_i[i_plot], '--', color='k',\n",
    "            label='Masked spectrum', lw=1.5)\n",
    "\n",
    "    # blank out masked regions\n",
    "    mask_i = mask[i]\n",
    "    spectra_i[mask_i] = np.nan\n",
    "\n",
    "    # plot the raw masked spectrum\n",
    "    ax.plot(wavelength, spectra_i[i_plot], '-', color='k',\n",
    "            label='True spectrum', lw=1.5)\n",
    "\n",
    "    # plot two levels of reconstruction\n",
    "    for nev in [0,2]:\n",
    "        if nev == 0:\n",
    "            label = 'mean'\n",
    "            color='red'\n",
    "        else:\n",
    "            label = 'reconstruction\\n(nev=%i)' % nev\n",
    "            color='grey'\n",
    "        spectra_i_recons = norms[i] * (mu + np.dot(coeffs[:nev], evecs[:nev]))\n",
    "        ax.plot(wavelength, spectra_i_recons[i_plot], label=label, color=color)\n",
    "\n",
    "    # plot shaded background in masked region\n",
    "    ylim = ax.get_ylim()\n",
    "    mask_shade = ylim[0] + mask[i][i_plot].astype(float) * ylim[1]\n",
    "    plt.fill(np.concatenate([wavelength[:1], wavelength, wavelength[-1:]]),\n",
    "             np.concatenate([[ylim[0]], mask_shade, [ylim[0]]]),\n",
    "             lw=0, fc='k', alpha=0.2)\n",
    "\n",
    "    ax.set_xlim(wavelength[0], wavelength[-1])\n",
    "    ax.set_ylim(ylim)\n",
    "    #ax.yaxis.set_major_formatter(ticker.NullFormatter())\n",
    "\n",
    "    if subplot == 311:\n",
    "        ax.legend(loc=1)\n",
    "\n",
    "    ax.set_xlabel('$\\lambda\\ (\\AA)$')\n",
    "    ax.set_ylabel('normalized flux')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing PCA, NMF and ICA  <a id='all3'></a>\n",
    "[Go to top](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonnegative Matrix Factorization (NMF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non negative factorization\n",
    "\n",
    "Eigenvectors are defined relative to a mean data vector. Principal components that can be positive or negative but for many physical systems we know data are a linear sum of positive components (e.g. galaxy spectrum is a linear sum of stellar components)\n",
    "\n",
    "\n",
    "Nonnegative matrix factorization (NMF) applies positivity constraint\n",
    "\n",
    "Assumes that the data matrix $X$ is a product of two positive matrices, $X=Y*W$.\n",
    "\n",
    "Solved iteratively for $Y$ and $W$ by minimizing the reconstruction error $||(X-WY)^2||$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "X = np.random.random((100, 3))  # 100 points in 3 dims, all positive\n",
    "nmf = NMF(n_components=3)  # setting n_components is optional\n",
    "nmf.fit(X)\n",
    "proj = nmf.transform(X)  # project to 3 dimensions\n",
    "\n",
    "comp = nmf.components_  # 3 x 10 array of components\n",
    "err = nmf.reconstruction_err_  # how well 3 components captures data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Independent component analysis\n",
    "\n",
    "Independent component analysis (ICA) solves the ``cocktail party problem'', where there are multiple microphones situated through out a room containing\n",
    "$N$ people. Each microphone picks up a linear combination of the $N$\n",
    "voices. The goal of ICA is to use the concept of statistical\n",
    "independence to isolate (or unmix) the individual signals. \n",
    "\n",
    "Each spectrum, $x_i(k)$, can be described by\n",
    "\n",
    ">$x_1(k) = a_{11}s_1(k) + a_{12}s_2(k) + a_{13}s_3(k) + \\cdots\\\\\n",
    "x_2(k) = a_{21}s_1(k) + a_{22}s_2(k) + a_{23}s_3(k) + \\cdots \\\\\n",
    "x_3(k) = a_{31}s_1(k) + a_{32}s_2(k) + a_{33}s_3(k) + \\cdots \\\\\n",
    "$\n",
    "\n",
    "where $s_i(k)$ are the individual  spectra and $a_{ij}$\n",
    "the appropriate mixing amplitudes. \n",
    "\n",
    "In matrix form,\n",
    ">$ X = A S$,\n",
    "\n",
    " Extracting these signal spectra is equivalent to estimating the appropriate weight matrix,\n",
    "$W$, such that\n",
    ">$ S = W X$.\n",
    "\n",
    "\n",
    "Underlying ICA is the observation that the\n",
    "input signals, $s_i(k)$, should be **statistically\n",
    "independent** - the joint probability distribution, $f(x,y)$, can be\n",
    "fully described by  $f(x^p,y^q)=f(x^p)f(y^q)$\n",
    "\n",
    " The rationale for ICA is that the\n",
    "sum of any two independent random variables will always be more\n",
    "Gaussian than either of the individual random variables (i.e., from the\n",
    "central limit theorem). If we identify an unmixing matrix, $W$, that\n",
    "maximizes the non-Gaussianity of the distributions, then we would be\n",
    "identifying the input signals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import FastICA\n",
    "\n",
    "X = np.random.normal(size=(100, 2))  # 100 pts in 2 dims\n",
    "R = np.random.random((2, 5))  # mixing matrix\n",
    "X = np.dot(X, R)  # X is now 2D data in 5D space\n",
    "ica = FastICA(2)  # fit two components\n",
    "\n",
    "sources = ica.fit_transform(X)\n",
    "proj = ica.transform(X)  # 100 x 2 projection of data\n",
    "comp = ica.components_  # the 2 x 5 matrix of indep. components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing PCA, NMF and ICA\n",
    "\n",
    "A comparison of the decomposition of SDSS spectra using PCA (left panel), ICA (middle panel) and \n",
    "NMF (right panel). The rank of the component increases from top to bottom. For the ICA and PCA \n",
    "the first component is the mean spectrum (NMF does not require mean subtraction). All of these \n",
    "techniques isolate a common set of spectral features (identifying features associated with the \n",
    "continuum and line emission). The ordering of the spectral components is technique dependent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF, FastICA, PCA\n",
    "\n",
    "data = sdss_corrected_spectra.fetch_sdss_corrected_spectra()\n",
    "spectra = sdss_corrected_spectra.reconstruct_spectra(data)\n",
    "wavelengths = sdss_corrected_spectra.compute_wavelengths(data)\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# Compute PCA, ICA, and NMF components\n",
    "def compute_PCA_ICA_NMF(n_components=5):\n",
    "    spec_mean = spectra.mean(0)\n",
    "\n",
    "    # PCA: \n",
    "    pca = PCA(n_components - 1, random_state=0, svd_solver='randomized')\n",
    "    pca.fit(spectra)\n",
    "    pca_comp = np.vstack([spec_mean,\n",
    "                          pca.components_])\n",
    "\n",
    "    # ICA treats sequential observations as related.  Because of this, we need\n",
    "    # to fit with the transpose of the spectra\n",
    "    ica = FastICA(n_components - 1, random_state=0)\n",
    "    ica.fit(spectra.T)\n",
    "    ica_comp = np.vstack([spec_mean,\n",
    "                          ica.transform(spectra.T).T])\n",
    "\n",
    "    # NMF requires all elements of the input to be greater than zero\n",
    "    spectra[spectra < 0] = 0\n",
    "    nmf = NMF(n_components, random_state=0)\n",
    "    nmf.fit(spectra)\n",
    "    nmf_comp = nmf.components_\n",
    "\n",
    "    return pca_comp, ica_comp, nmf_comp\n",
    "\n",
    "n_components = 5\n",
    "decompositions = compute_PCA_ICA_NMF(n_components)\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# Plot the results\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "fig.subplots_adjust(left=0.05, right=0.95, wspace=0.05,\n",
    "                    bottom=0.1, top=0.95, hspace=0.05)\n",
    "\n",
    "titles = ['PCA components', 'ICA components', 'NMF components']\n",
    "\n",
    "for i, comp in enumerate(decompositions):\n",
    "    for j in range(n_components):\n",
    "        ax = fig.add_subplot(n_components, 3, 3 * j + 1 + i)\n",
    "\n",
    "        ax.yaxis.set_major_formatter(plt.NullFormatter())\n",
    "        ax.xaxis.set_major_locator(plt.MultipleLocator(1000))\n",
    "        if j < n_components - 1:\n",
    "            ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "        else:\n",
    "            ax.xaxis.set_major_locator(\n",
    "                plt.FixedLocator(list(range(3000, 7999, 1000))))\n",
    "            ax.set_xlabel(r'wavelength ${\\rm (\\AA)}$')\n",
    "\n",
    "        ax.plot(wavelengths, comp[j], '-k', lw=1)\n",
    "\n",
    "        # plot zero line\n",
    "        xlim = [3000, 8000]\n",
    "        ax.plot(xlim, [0, 0], '-', c='gray', lw=1)\n",
    "\n",
    "        if j == 0:\n",
    "            ax.set_title(titles[i])\n",
    "\n",
    "        if titles[i].startswith('PCA') or titles[i].startswith('ICA'):\n",
    "            if j == 0:\n",
    "                label = 'mean'\n",
    "            else:\n",
    "                label = 'component %i' % j\n",
    "        else:\n",
    "            label = 'component %i' % (j + 1)\n",
    "\n",
    "        ax.text(0.03, 0.94, label, transform=ax.transAxes,\n",
    "                ha='left', va='top')\n",
    "\n",
    "        for l in ax.get_xticklines() + ax.get_yticklines():\n",
    "            l.set_markersize(2)\n",
    "\n",
    "        # adjust y limits\n",
    "        ylim = plt.ylim()\n",
    "        dy = 0.05 * (ylim[1] - ylim[0])\n",
    "\n",
    "        ax.set_ylim(ylim[0] - dy, ylim[1] + 4 * dy)\n",
    "        ax.set_xlim(xlim)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manifold Learning <a id='LLE'></a>\n",
    "\n",
    "[Go to top](#toc)\n",
    "\n",
    "Real-world data sets often have very nonlinear features (e.g. QSOs with broad lines) which are hard to describe compactly using linear eigenvectors. Manifold learning techniques search for a representation of these data within a lower dimensional space\n",
    "\n",
    "<img src=\"http://www.astroml.org/_images/fig_S_manifold_PCA_1.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Local Linear Embedding:\n",
    "\n",
    "Step 1: define local geometry\n",
    "- local neighborhoods determined from $k$ nearest neighbors.\n",
    "- for each point calculate weights that reconstruct a point from its $k$ nearest\n",
    "neighbors\n",
    ">$\n",
    "\\begin{equation}\n",
    "  \\mathcal{E}_1(W) = \\left|X - WX\\right|^2\n",
    "\\end{equation}\n",
    "$\n",
    "- essentially this is finding the hyperplane that describes the local surface at each point within the data set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: embed within a lower dimensional space\n",
    "- set all $W_{ij}=0$ except when point $j$ is one of the $k$ nearest neighbors of point $i$.  \n",
    "- $W$ becomes very sparse for $k \\ll N$.  \n",
    "- minimize\n",
    ">$\\begin{equation}\n",
    "  \\mathcal{E}_2(Y) = \\left|Y - W Y\\right|^2,\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "with $W$ fixed to find an $N$ by $d$ matrix ($d$ is the new dimensionality)\n",
    "\n",
    "Step 1 requires a nearest-neighbor search.\n",
    "\n",
    "Step 2 requires an\n",
    "eigenvalue decomposition of the matrix $C_W \\equiv (I-W)^T(I-W)$,\n",
    "\n",
    "\n",
    "LLE has been applied to data as diverse as galaxy spectra, stellar spectra, and photometric light curves. \n",
    "\n",
    "[Survey of LLE variants](https://arxiv.org/pdf/2011.10925.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astroML.plotting.tools import discretize_cmap\n",
    "from astroML.decorators import pickle_results\n",
    "from sklearn import manifold, neighbors\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# This function adjusts matplotlib settings for a uniform feel in the textbook.\n",
    "# Note that with usetex=True, fonts are rendered with LaTeX.  This may\n",
    "# result in an error if LaTeX is not installed on your system.  In that case,\n",
    "# you can set usetex to False.\n",
    "from astroML.plotting import setup_text_plots\n",
    "setup_text_plots(fontsize=8, usetex=True)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Set up color-map properties\n",
    "clim = (1.5, 6.5)\n",
    "cmap = discretize_cmap(plt.cm.jet, 5)\n",
    "cdict = ['unknown', 'star', 'absorption galaxy',\n",
    "         'galaxy', 'emission galaxy',\n",
    "         'narrow-line QSO', 'broad-line QSO']\n",
    "cticks = [2, 3, 4, 5, 6]\n",
    "formatter = plt.FuncFormatter(lambda t, *args: cdict[int(np.round(t))])\n",
    "\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Fetch the data; PCA coefficients have been pre-computed\n",
    "spec = sdss_corrected_spectra.reconstruct_spectra(data)\n",
    "color = data['lineindex_cln']\n",
    "\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Compute the LLE projection; save the results\n",
    "def compute_spec_LLE(n_neighbors=10, out_dim=3):\n",
    "    # Compute the LLE projection\n",
    "    LLE = manifold.LocallyLinearEmbedding(n_neighbors=n_neighbors, \n",
    "                                          n_components=out_dim,\n",
    "                                          method='modified',\n",
    "                                          eigen_solver='dense')\n",
    "    Y_LLE = LLE.fit_transform(spec)\n",
    "    print (\" - finished LLE projection\")\n",
    "\n",
    "    # remove outliers for the plot\n",
    "    BT = neighbors.BallTree(Y_LLE)\n",
    "    dist, ind = BT.query(Y_LLE, n_neighbors)\n",
    "    dist_to_n = dist[:, -1]\n",
    "    dist_to_n -= dist_to_n.mean()\n",
    "    std = np.std(dist_to_n)\n",
    "    flag = (dist_to_n > 0.25 * std)\n",
    "    print (\" - removing %i outliers for plot\" % flag.sum())\n",
    "\n",
    "    return Y_LLE[~flag], color[~flag]\n",
    "\n",
    "coeffs_LLE, c_LLE = compute_spec_LLE(10, 3)\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# Plot the results:\n",
    "for (c, coeffs, xlim) in zip([c_LLE],\n",
    "                             [coeffs_LLE],\n",
    "                             [(-0.01, 0.014)]):\n",
    "    fig = plt.figure(figsize=(5, 5))\n",
    "    fig.subplots_adjust(hspace=0.05, wspace=0.05)\n",
    "\n",
    "    # axes for colorbar\n",
    "    cax = plt.axes([0.525, 0.525, 0.02, 0.35])\n",
    "\n",
    "    # Create scatter-plots\n",
    "    scatter_kwargs = dict(s=4, lw=0, edgecolors='none', c=c, cmap=cmap)\n",
    "\n",
    "    ax1 = plt.subplot(221)\n",
    "    im1 = ax1.scatter(coeffs[:, 0], coeffs[:, 1], **scatter_kwargs)\n",
    "    im1.set_clim(clim)\n",
    "    ax1.set_ylabel('$c_2$')\n",
    "\n",
    "    ax2 = plt.subplot(223)\n",
    "    im2 = ax2.scatter(coeffs[:, 0], coeffs[:, 2], **scatter_kwargs)\n",
    "    im2.set_clim(clim)\n",
    "    ax2.set_xlabel('$c_1$')\n",
    "    ax2.set_ylabel('$c_3$')\n",
    "\n",
    "    ax3 = plt.subplot(224)\n",
    "    im3 = ax3.scatter(coeffs[:, 1], coeffs[:, 2], **scatter_kwargs)\n",
    "    im3.set_clim(clim)\n",
    "    ax3.set_xlabel('$c_2$')\n",
    "\n",
    "    fig.colorbar(im3, ax=ax3, cax=cax,\n",
    "                 ticks=cticks,\n",
    "                 format=formatter)\n",
    "\n",
    "    ax1.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "    ax3.yaxis.set_major_formatter(plt.NullFormatter())\n",
    "\n",
    "    ax1.set_xlim(xlim)\n",
    "    ax2.set_xlim(xlim)\n",
    "\n",
    "    for ax in (ax1, ax2, ax3):\n",
    "        ax.xaxis.set_major_locator(plt.MaxNLocator(5))\n",
    "        ax.yaxis.set_major_locator(plt.MaxNLocator(5))\n",
    "\n",
    "plt.show()\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary of pracical properties of PCA, NMF, ICA, and LLE\n",
    "\n",
    "The following table is a simple summary of the trade-offs along our axes of\n",
    "accuracy, interpretability, simplicity, and speed in dimension\n",
    "reduction methods, expressed in terms of high (H), medium\n",
    "(M), and low (L) categories.\n",
    "\n",
    "|Method | Accuracy | Interpretability | Simplicity | Speed |\n",
    "|-------|----------|------------------|------------|-------|\n",
    "|Principal component analysis | H | H | H |  H |\n",
    "|Nonnegative matrix factorization | H | H | M | M|\n",
    "|Independent component analysis | M | M | L | L |\n",
    "|Locally linear embedding | H | M | H | H |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: for a set of spectra compare the coefficients from LLE and PCA coefficients by plotting the coefficicents  as a function of galaxy type\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the entries in the SDSS spectroscpic data set\n",
    "for k in data.iterkeys():\n",
    "    print (k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### spec_cln\n",
    "\n",
    "- SPEC_UNKNOWN = 0\n",
    "- SPEC_STAR    = 1\n",
    "- SPEC_GALAXY  = 2\n",
    "- SPEC_QSO     = 3\n",
    "- SPEC_HIZ_QSO = 4 /* high redshift QSO, z>2.3, Ly-alpha finding code is triggered */ \n",
    "- SPEC_SKY     = 5 \n",
    "- STAR_LATE    = 6 /* star dominated by molecular bands, Type M or later           */\n",
    "- GAL_EM       = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### lineindex_cln\n",
    "\n",
    "- 0 : unknown (SPEC_CLN = 0)\n",
    "- 1 : star (SPEC_CLN = 1)\n",
    "- 2 : absorption galaxy (H-alpha seen in absorption)\n",
    "- 3 : normal galaxy (no significant H-alpha emission or absorption)\n",
    "- 4 : emission line galaxies (below line-ratio curve)\n",
    "- 5 : narrow-line QSO (above line-ratio curve)\n",
    "- 6 : broad-line QSO (SPEC_CLN = 3)\n",
    "- 7 : Sky (SPEC_CLN = 4)\n",
    "- 8 : Hi-z QSO (SPEC_CLN = 5)\n",
    "- 9 : Late-type star (SPEC_CLN = 6)\n",
    "- 10 : Emission galaxy (SPEC_CLN = 7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

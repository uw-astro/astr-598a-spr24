{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### University of Washington: Machine Learning and Statistics \n",
    "\n",
    "# Lecture 7:  Density Estimation 2\n",
    "\n",
    "Andrew Connolly and Stephen Portillo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Resources for this notebook include:\n",
    "- [Textbook](https://press.princeton.edu/books/hardcover/9780691198309/statistics-data-mining-and-machine-learning-in-astronomy) Chapter 8. \n",
    "- [astroML website](https://www.astroml.org/index.html)\n",
    "\n",
    "This notebook is developed based on material from A. Connolly, Z. Ivezic, M. Juric, S. Portillo, G. Richards, B. Sipocz, J. VanderPlas, D. Hogg, Killian Weinberger and many others.\n",
    "\n",
    "The notebook and assoociated material are available from [github](https://github.com/uw-astro/astr-598a-win22).\n",
    "\n",
    "Make sure you are using the latest version of astroML\n",
    "\n",
    "> pip install --pre -U astroml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Resources for this notebook include:\n",
    "- [Textbook](http://press.princeton.edu/titles/10159.html) Chapters 6 and 9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc'></a>\n",
    "\n",
    "## This notebook includes:\n",
    " \n",
    "\n",
    "[Mixture Models: Gaussian](#gmm)\n",
    "\n",
    "[How do we choose the number of components](#comp)\n",
    "\n",
    "[Extreme deconvolution (XD)](#xd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixture Models: Gaussian <a id='gmm'></a>\n",
    "[Go to top](#toc)\n",
    "\n",
    "\n",
    "Mixture models use the sum of functions to represent the density distributions - used in defining the density, classifications, cloning of a data set....\n",
    "\n",
    "Gaussian mixture models (GMMs) are the most common implementation of mixture models\n",
    "\n",
    "$\\rho(\\mathbf{x}) = N\\, p(\\mathbf{x})\n",
    "  = N\\, \\sum_{j=1}^M \\alpha_j \\mathcal{N}(\\mu_j, \\Sigma_j)$\n",
    "  \n",
    "with  $p(\\mathbf{x}) = \\sum_j \\alpha_j \\mathcal{N}(\\mathbf{x}|\\mathbf{\\mu}_{j},\\mathbf{\\Sigma}_{j})$\n",
    "\n",
    "and\n",
    "\n",
    "$\\mathcal{N}(\\mathbf{x}|\\mathbf{\\mu}_j,\\mathbf{\\Sigma}_j) =\n",
    "  \\frac{1}{\\sqrt{(2\\pi)^D\\mbox{det}(\\mathbf{\\Sigma}_j)}}\n",
    "      \\exp\\Big(-\\frac{1}{2}(\\mathbf{x}-\\mathbf{\\mu})^T\\mathbf{\\Sigma}_j^{-1}(\\mathbf{x}-\\mathbf{\\mu})\\Big)\\, $\n",
    "\n",
    "\n",
    "\n",
    "where the model consists of $N$ Gaussians with locations $\\mu_j$\n",
    "and covariances $\\Sigma_j$. The log-likelihood is straightforward to define by this model.\n",
    "\n",
    "<u> _Expectation maximization_ </u> is typically employed to solve for the mixture of Gaussians\n",
    "\n",
    "\n",
    "- _Expectation_\n",
    "\n",
    "  - Given a set of Gaussians compute the “expected” classes of all points\n",
    "\n",
    "- _Maximization_\n",
    "\n",
    "  - Estimate the MLE of $\\mu$, amplitude, and $\\Sigma$ given the data’s class membership \n",
    "\n",
    "Iterative proceedure until variance does not change. Guaranteed to converge -  monotonically approaches a local minimum of the cost function (but guaranteed to be the correct answer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import norm\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from astroML.utils import convert_2D_cov\n",
    "from astroML.plotting.tools import draw_ellipse\n",
    "\n",
    "def generate_data(clusters, Npts):\n",
    "    '''generate data set given set of gaussians'''\n",
    "    # Set the internal means, covariances, and weights by-hand.\n",
    "    gmm_input = GaussianMixture(len(clusters), covariance_type='full')\n",
    "    gmm_input.means_ = np.array([c[0] for c in clusters])\n",
    "    gmm_input.covariances_ = np.array([convert_2D_cov(*c[1:4]) for c in clusters])\n",
    "    gmm_input.weights_ = np.array([c[4] for c in clusters])\n",
    "    gmm_input.weights_ /= gmm_input.weights_.sum()\n",
    "    gmm_input.precisions_cholesky_ = 1 / np.sqrt(gmm_input.covariances_)\n",
    "    gmm_input.fit = None\n",
    "    return gmm_input.sample(Npts)[0]\n",
    "\n",
    "# define clusters as (mu, sigma1, sigma2, alpha, frac)\n",
    "Npts=1000\n",
    "clusters = [((50, 50), 20, 20, 0, 0.1),\n",
    "        ((40, 40), 10, 10, np.pi / 6, 0.6),\n",
    "        ((80, 80), 5, 5, np.pi / 3, 0.2),\n",
    "        ((60, 60), 30, 30, 0, 0.1)]\n",
    "\n",
    "X = generate_data(clusters, Npts)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Compute and plot the results\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "\n",
    "def plot_gmm_solution(ax, clf, iterations):  \n",
    "    ax.plot(X[:, 0], X[:, 1], '.', c='red', ms=1, zorder=1)\n",
    "    ax.set_xlim(X[:,0].min(), X[:,0].max())\n",
    "    ax.set_ylim(X[:,1].min(), X[:,1].max())\n",
    "    ax.set_ylabel('$y$')\n",
    "    ax.set_xlabel('$x$')\n",
    "    \n",
    "    for i in range(clf.n_components):\n",
    "        mean = clf.means_[i]\n",
    "        cov = clf.covariances_[i]\n",
    "        if cov.ndim == 1:\n",
    "            cov = np.diag(cov)\n",
    "        draw_ellipse(mean, cov, ax=ax, fc='none', ec='k',ls=linestyles[i], zorder=2)\n",
    "    \n",
    "    ax.text(0.05, 0.95, \"%i iterations\" % iterations,\n",
    "        ha='left', va='top', transform=ax.transAxes,\n",
    "        bbox=dict(fc='w', ec='k'))\n",
    "\n",
    " \n",
    "linestyles = ['-', '--', ':']\n",
    "np.random.seed(1)\n",
    "#plot solutions as a function of iteration\n",
    "def fitAndPlot(X, n_components=1, n_iter=100):\n",
    "    \n",
    "    for i,iter in enumerate(np.logspace(0, np.log10(n_iter), 4, dtype=int)):\n",
    "        clf = GaussianMixture(n_components, random_state=np.random.seed(1), \n",
    "                              max_iter=iter, init_params='random').fit(X)\n",
    "\n",
    "        ax = fig.add_subplot(2,2,i+1)\n",
    "        plot_gmm_solution(ax, clf, iter)\n",
    "  \n",
    "fitAndPlot(X, n_components=3, n_iter=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do we choose the number of components? <a id='comp'></a>\n",
    "[Go to top](#toc)\n",
    "\n",
    "\n",
    "Typically used as a density estimation and not as a way of determining the number of Gaussians in a distribution (e.g. imagine what would happen with a single cluster and a uniform background)\n",
    "\n",
    "Both **Akaike information criterion (AIC)** and **Bayesian information criterion (BIC)** are scoring systems for model comparisons in classical statistics dealing with models with different numbers of free parameters.  \n",
    "\n",
    "Specifically, **AIC** is computed as\n",
    "\n",
    "$$AIC \\equiv -2 ln(L^0(M)) + 2k + \\frac{2k(k+1)}{N-k-1}$$  \n",
    "\n",
    "**BIC** is computed as\n",
    "\n",
    "$$BIC \\equiv -2ln[L^0(M)] + k lnN$$  \n",
    "  \n",
    "AIC, BIC, and cross-validation are often used to define the number of parameters (though this is rarely well defined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "from itertools import cycle\n",
    "\n",
    "from astroML.datasets import fetch_sdss_sspp\n",
    "from astroML.decorators import pickle_results\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Get the Segue Stellar Parameters Pipeline data\n",
    "data = fetch_sdss_sspp(cleaned=True)\n",
    "X = np.vstack([data['FeH'], data['alphFe']]).T\n",
    "\n",
    "# truncate dataset for speed\n",
    "X = X[::5]\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Compute GMM models & AIC/BIC\n",
    "N = np.linspace(1, 50, 50, dtype=int)\n",
    "\n",
    "def compute_GMM(N, covariance_type='full', n_iter=1000):\n",
    "    models = [None for n in N]\n",
    "    for i in range(len(N)):\n",
    "        models[i] = GaussianMixture(N[i], max_iter=500).fit(X)\n",
    "    return models\n",
    "\n",
    "models = compute_GMM(N)\n",
    "\n",
    "AIC = [m.aic(X) for m in models]\n",
    "BIC = [m.bic(X) for m in models]\n",
    "\n",
    "i_best = np.argmin(BIC)\n",
    "gmm_best = models[i_best]\n",
    "print (\"best fit converged:\", gmm_best.converged_)\n",
    "print (\"n_components =  %i\" % N[i_best])\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# compute 2D density\n",
    "FeH_bins = 51\n",
    "alphFe_bins = 51\n",
    "H, FeH_bins, alphFe_bins = np.histogram2d(data['FeH'], data['alphFe'],\n",
    "                                          (FeH_bins, alphFe_bins))\n",
    "\n",
    "Xgrid = np.array(list(map(np.ravel,\n",
    "                     np.meshgrid(0.5 * (FeH_bins[:-1]\n",
    "                                        + FeH_bins[1:]),\n",
    "                                 0.5 * (alphFe_bins[:-1]\n",
    "                                        + alphFe_bins[1:]))))).T\n",
    "log_dens = gmm_best.score_samples(Xgrid).reshape((51, 51))\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Plot the results\n",
    "fig = plt.figure(figsize=(19, 6))\n",
    "fig.subplots_adjust(wspace=0.4,\n",
    "                    bottom=0.2, top=0.9,\n",
    "                    left=0.1, right=0.95)\n",
    "\n",
    "# plot density\n",
    "ax = fig.add_subplot(141)\n",
    "ax.imshow(H.T, origin='lower', interpolation='nearest', aspect='auto',\n",
    "          extent=[FeH_bins[0], FeH_bins[-1],\n",
    "                  alphFe_bins[0], alphFe_bins[-1]],\n",
    "          cmap=plt.cm.binary)\n",
    "ax.set_xlabel(r'$\\rm [Fe/H]$')\n",
    "ax.set_ylabel(r'$\\rm [\\alpha/Fe]$')\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(0.3))\n",
    "ax.set_xlim(-1.101, 0.101)\n",
    "ax.text(0.96, 0.96, \"Input\\nDistribution\", fontsize=12,\n",
    "        va='top', ha='right', transform=ax.transAxes)\n",
    "\n",
    "# plot AIC/BIC\n",
    "ax = fig.add_subplot(142)\n",
    "ax.plot(N, AIC, '-k', label='AIC')\n",
    "ax.plot(N, BIC, '--k', label='BIC')\n",
    "ax.legend(loc=1, prop=dict(size=12))\n",
    "ax.set_xlabel('N components')\n",
    "plt.setp(ax.get_yticklabels(), fontsize=12)\n",
    "\n",
    "# plot best configurations for AIC and BIC\n",
    "ax = fig.add_subplot(143)\n",
    "ax.imshow(np.exp(log_dens),\n",
    "          origin='lower', interpolation='nearest', aspect='auto',\n",
    "          extent=[FeH_bins[0], FeH_bins[-1],\n",
    "                  alphFe_bins[0], alphFe_bins[-1]],\n",
    "          cmap=plt.cm.binary)\n",
    "\n",
    "linestyles = ['-', '--', ':']\n",
    "# plot best configurations for AIC and BIC\n",
    "ax.scatter(gmm_best.means_[:, 0], gmm_best.means_[:, 1], c='w')\n",
    "for i in range(gmm_best.n_components):\n",
    "    mean = gmm_best.means_[i]\n",
    "    cov = gmm_best.covariances_[i]\n",
    "    if cov.ndim == 1:\n",
    "        cov = np.diag(cov)\n",
    "    draw_ellipse(mean, cov, ax=ax, fc='none', ec='k', zorder=2)\n",
    "    \n",
    "ax.text(0.96, 0.96, \"Converged\\nconfiguration\", fontsize=12,\n",
    "        va='top', ha='right', transform=ax.transAxes)\n",
    "\n",
    "ax.set_xlim(-1.101, 0.101)\n",
    "ax.set_ylim(alphFe_bins[0], alphFe_bins[-1])\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(0.3))\n",
    "ax.set_xlabel(r'$\\rm [Fe/H]$')\n",
    "ax.set_ylabel(r'$\\rm [\\alpha/Fe]$')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: clustering of orbital data for asteroids\n",
    "\n",
    "Asteroids are clustered in orbital parameter space (6 dimensions\n",
    "but here we will consider two: semi-major axis and inclination)\n",
    "and these clusters, known as families, are believed to be remnants of larger asteroids destroyed in collisions. Typically, families have\n",
    "uniform colors, for more details see [Parker et al. 2008.](\n",
    "http://faculty.washington.edu/ivezic/Publications/parker.pdf)\n",
    "\n",
    "Using the Parker et al. dataset, available from astroML, apply \n",
    "Gaussian Mixture Model estimate the number of components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch data and extract the desired quantities\n",
    "from astroML.datasets import fetch_moving_objects\n",
    "dataAll = fetch_moving_objects(Parker2008_cuts=True)\n",
    "data = dataAll[1:]\n",
    "a = data['aprime']\n",
    "sini = data['sin_iprime']\n",
    "acolor = data['mag_a']\n",
    "izcolor = data['mag_i'] - data['mag_z']\n",
    "X = np.vstack([a, sini]).T\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Plot \n",
    "xmin, xmax = (2.0, 3.3)\n",
    "ymin, ymax = (0.0, 0.33)\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "fig.subplots_adjust(hspace=0, left=0.1, right=0.95, bottom=0.1, top=0.9)\n",
    "\n",
    "ax = fig.add_subplot(311)\n",
    "ax.scatter(X[:, 0], X[:, 1], s=1, lw=0.5, c='k')\n",
    "ax.set_xlim(xmin, xmax)\n",
    "ax.set_ylim(ymin, ymax)\n",
    "ax.set_xlabel('a (AU)')\n",
    "ax.set_ylabel('sin(i)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Parker et al. asteroid families](figures/MOC4_population_labels.jpg) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extreme deconvolution (XD) <a id='xd'></a>\n",
    "[Go to top](#toc)\n",
    "\n",
    "\n",
    "Combines Baysian extimation and Gaussian model in the algorithm to correct data with measurement errors. The algorithm basically assumes an observed value ${x_i}$, true value ${v_i}$, and noise ${\\epsilon}$ \n",
    "have the following relationship:\n",
    "\n",
    "$${x_i} = {R_i}{v_i}+{\\epsilon_i}$$  \n",
    "\n",
    "where ${R_i}$ is a projection matrix. After we compute this matrix using assumed Gaussian model, we are able to convert\n",
    "noisy data back to true data.  \n",
    "  \n",
    "We will start with a simulated data set to show  how XD corrects noisy unsupervised data. \n",
    "\n",
    "We then apply to real stellar data sample and see how XD works in correcting data distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XD: Simulated data\n",
    "\n",
    "We first generate a distribution of \"true data\" using radomized numbers. We will plot this data set as a reference to compare with the model derived from estimation with XD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample the dataset. \n",
    "# Here we use sample size = 400 in the example, \n",
    "# which converges in shorter time, and gives reasonable result.\n",
    "N = 400\n",
    "np.random.seed(0)\n",
    "\n",
    "# generate the true data\n",
    "x_true = (1.4 + 2 * np.random.random(N)) ** 2\n",
    "y_true = 0.1 * x_true ** 2\n",
    "\n",
    "# add scatter to \"true\" distribution\n",
    "dx = 0.1 + 4. / x_true ** 2\n",
    "dy = 0.1 + 10. / x_true ** 2\n",
    "\n",
    "x_true += np.random.normal(0, dx, N)\n",
    "y_true += np.random.normal(0, dy, N)\n",
    "\n",
    "# add noise to get the \"observed\" distribution\n",
    "dx = 0.2 + 0.5 * np.random.random(N)\n",
    "dy = 0.2 + 0.5 * np.random.random(N)\n",
    "\n",
    "x = x_true + np.random.normal(0, dx)\n",
    "y = y_true + np.random.normal(0, dy)\n",
    "\n",
    "\n",
    "# define a function to plot all distributions in the same format\n",
    "def plot_distribution(ax, text, sample_x, sample_y):\n",
    "    ax.scatter(sample_x, sample_y, s=4,lw=0,c='k')\n",
    "    ax.set_xlim(-1, 13)\n",
    "    ax.set_ylim(-6, 16)\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$y$')\n",
    "    ax.set_title(text,fontsize=10)\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "# plot true distribution\n",
    "ax = fig.add_subplot(121)\n",
    "plot_distribution(ax, 'True Distribution', x_true, y_true)\n",
    "# plot noisy distribution\n",
    "ax = fig.add_subplot(122)\n",
    "plot_distribution(ax, 'Noisy Distribution', x, y)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute extreme deconvolution (XD)\n",
    "In the XDGMM method, n_components (integer) defines the number of Gaussian components to fit to the data. max_iter (integer) defines number of EM iterations to perform (default as 100). Larger iteration number generally contributes better approximation to the true data, but takes longer time to execute. This cell is expected to execute in a bit long time around 1 minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astroML.density_estimation import XDGMM\n",
    "\n",
    "# stack the results for computation\n",
    "X = np.vstack([x, y]).T\n",
    "Xerr = np.zeros(X.shape + X.shape[-1:])\n",
    "diag = np.arange(X.shape[-1])\n",
    "Xerr[:, diag, diag] = np.vstack([dx ** 2, dy ** 2]).T\n",
    "\n",
    "clf = XDGMM(n_components=10, max_iter=200)\n",
    "\n",
    "clf.fit(X, Xerr)\n",
    "sample = clf.sample(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "fig = plt.figure(figsize=(5, 3.75))\n",
    "fig.subplots_adjust(left=0.1, right=0.95,\n",
    "                    bottom=0.1, top=0.95,\n",
    "                    wspace=0.02, hspace=0.02)\n",
    "\n",
    "ax1 = fig.add_subplot(221)\n",
    "ax1.scatter(x_true, y_true, s=4, lw=0, c='k')\n",
    "\n",
    "ax2 = fig.add_subplot(222)\n",
    "ax2.scatter(x, y, s=4, lw=0, c='k')\n",
    "\n",
    "ax3 = fig.add_subplot(223)\n",
    "ax3.scatter(sample[:, 0], sample[:, 1], s=4, lw=0, c='k')\n",
    "\n",
    "ax4 = fig.add_subplot(224)\n",
    "for i in range(clf.n_components):\n",
    "    draw_ellipse(clf.mu[i], clf.V[i], scales=[2], ax=ax4,\n",
    "                 ec='k', fc='gray', alpha=0.2)\n",
    "\n",
    "titles = [\"True Distribution\", \"Noisy Distribution\",\n",
    "          \"Extreme Deconvolution\\n  resampling\",\n",
    "          \"Extreme Deconvolution\\n  cluster locations\"]\n",
    "\n",
    "ax = [ax1, ax2, ax3, ax4]\n",
    "\n",
    "for i in range(4):\n",
    "    ax[i].set_xlim(-1, 13)\n",
    "    ax[i].set_ylim(-6, 16)\n",
    "\n",
    "    ax[i].xaxis.set_major_locator(plt.MultipleLocator(4))\n",
    "    ax[i].yaxis.set_major_locator(plt.MultipleLocator(5))\n",
    "\n",
    "    ax[i].text(0.05, 0.95, titles[i],\n",
    "               ha='left', va='top', transform=ax[i].transAxes)\n",
    "\n",
    "    if i in (0, 1):\n",
    "        ax[i].xaxis.set_major_formatter(plt.NullFormatter())\n",
    "    else:\n",
    "        ax[i].set_xlabel('$x$')\n",
    "\n",
    "    if i in (1, 3):\n",
    "        ax[i].yaxis.set_major_formatter(plt.NullFormatter())\n",
    "    else:\n",
    "        ax[i].set_ylabel('$y$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XD on real star sample\n",
    "We use a set of standard stars from Stripe 82 (high SNR), and a set of noisy single epoch of stars (low SNR). We will perform XD on the noisy data and see how it resamples the data into clusters.\n",
    "\n",
    "First, we fix the star's true color from dust extinction in noisy data sample. We apply extinction correction curve defined as \n",
    "$C_{\\lambda} \\equiv \\frac{A_{\\lambda}}{A}$, where the value of $C_{\\lambda}$ for each band of S82 is from [Berry et al 2012](https://ui.adsabs.harvard.edu/abs/2012ApJ...757..166B/abstract)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astroML.datasets import fetch_sdss_S82standards, fetch_imaging_sample\n",
    "\n",
    "# define u-g-r-i-z extinction from Berry et al, arXiv 1111.4985 multiply extinction by A_r\n",
    "extinction_vector = np.array([1.810, 1.400, 1.0, 0.759, 0.561])\n",
    "\n",
    "# Fetch and process the noisy imaging data\n",
    "data_noisy = fetch_imaging_sample()\n",
    "\n",
    "# select only stars\n",
    "data_noisy = data_noisy[data_noisy['type'] == 6]\n",
    "\n",
    "# Get the extinction-corrected magnitudes for each band\n",
    "X = np.vstack([data_noisy[f + 'RawPSF'] for f in 'ugriz']).T\n",
    "Xerr = np.vstack([data_noisy[f + 'psfErr'] for f in 'ugriz']).T\n",
    "\n",
    "# extinction terms from Berry et al, arXiv 1111.4985\n",
    "X -= (extinction_vector * data_noisy['rExtSFD'][:, None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we stack the S82 star set and perform the same extinction correction on the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch and process the stacked imaging data\n",
    "data_stacked = fetch_sdss_S82standards()\n",
    "\n",
    "# cut to RA, DEC range of imaging sample\n",
    "RA = data_stacked['RA']\n",
    "DEC = data_stacked['DEC']\n",
    "data_stacked = data_stacked[(RA > 0) & (RA < 10) &\n",
    "                            (DEC > -1) & (DEC < 1)]\n",
    "\n",
    "# get stacked magnitudes for each band\n",
    "Y = np.vstack([data_stacked['mmu_' + f] for f in 'ugriz']).T\n",
    "Yerr = np.vstack([data_stacked['msig_' + f] for f in 'ugriz']).T\n",
    "\n",
    "# extinction terms from Berry et al, arXiv 1111.4985\n",
    "Y -= (extinction_vector * data_stacked['A_r'][:, None])\n",
    "\n",
    "# quality cuts\n",
    "g = Y[:, 1]\n",
    "mask = ((Yerr.max(1) < 0.05) &\n",
    "        (g < 20))\n",
    "data_stacked = data_stacked[mask]\n",
    "Y = Y[mask]\n",
    "Yerr = Yerr[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-match two data sets\n",
    "We use astroML's crossmatch to match the noisy sample to standard sample, and make two sets comparable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astroML.crossmatch import crossmatch\n",
    "\n",
    "Xlocs = np.hstack((data_noisy['ra'][:, np.newaxis],\n",
    "                   data_noisy['dec'][:, np.newaxis]))\n",
    "Ylocs = np.hstack((data_stacked['RA'][:, np.newaxis],\n",
    "                   data_stacked['DEC'][:, np.newaxis]))\n",
    "\n",
    "print(\"number of noisy points:  \", Xlocs.shape)\n",
    "print(\"number of stacked points:\", Ylocs.shape)\n",
    "\n",
    "# find all points within 0.9 arcsec.  This cutoff was selected\n",
    "# by plotting a histogram of the log(distances).\n",
    "dist, ind = crossmatch(Xlocs, Ylocs, max_distance=0.9 / 3600)\n",
    "\n",
    "noisy_mask = (~np.isinf(dist))\n",
    "stacked_mask = ind[noisy_mask]\n",
    "\n",
    "# select the data\n",
    "data_noisy = data_noisy[noisy_mask]\n",
    "X = X[noisy_mask]\n",
    "Xerr = Xerr[noisy_mask]\n",
    "\n",
    "data_stacked = data_stacked[stacked_mask]\n",
    "Y = Y[stacked_mask]\n",
    "Yerr = Yerr[stacked_mask]\n",
    "\n",
    "# double-check that our cross-match succeeded\n",
    "assert X.shape == Y.shape\n",
    "print(\"size after crossmatch:\", X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first define mixing matrix W\n",
    "W = np.array([[0, 1, 0, 0, 0],    # g magnitude\n",
    "              [1, -1, 0, 0, 0],   # u-g color\n",
    "              [0, 1, -1, 0, 0],   # g-r color\n",
    "              [0, 0, 1, -1, 0],   # r-i color\n",
    "              [0, 0, 0, 1, -1]])  # i-z color\n",
    "\n",
    "X = np.dot(X, W.T)\n",
    "Y = np.dot(Y, W.T)\n",
    "\n",
    "# compute error covariance from mixing matrix\n",
    "Xcov = np.zeros(Xerr.shape + Xerr.shape[-1:])\n",
    "Xcov[:, range(Xerr.shape[1]), range(Xerr.shape[1])] = Xerr ** 2\n",
    "\n",
    "# each covariance C = WCW^T\n",
    "# best way to do this is with a tensor dot-product\n",
    "Xcov = np.tensordot(np.dot(Xcov, W.T), W, (-2, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose ten percent of the points in each data sets to plot in comparison. As we can see from the result, before XD, the sigle epoch (right) has more noise than standard stars (left)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and sample from the underlying distribution\n",
    "np.random.seed(42)\n",
    "X_sample = clf.sample(X.shape[0])\n",
    "\n",
    "# plot the results\n",
    "fig = plt.figure(figsize=(10, 3.75))\n",
    "fig.subplots_adjust(left=0.12, right=0.95,\n",
    "                    bottom=0.1, top=0.95,\n",
    "                    wspace=0.02, hspace=0.02)\n",
    "\n",
    "# only plot 1/10 of the stars for clarity\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax1.scatter(Y[::10, 2], Y[::10, 3], s=9, lw=0, c='k')\n",
    "ax1.set_ylabel('$r-i$')\n",
    "\n",
    "ax2 = fig.add_subplot(122)\n",
    "ax2.scatter(X[::10, 2], X[::10, 3], s=9, lw=0, c='k')\n",
    "\n",
    "titles = [\"Standard Stars\", \"Single Epoch\"]\n",
    "ax = [ax1, ax2]\n",
    "\n",
    "for i in range(2):\n",
    "    ax[i].set_xlim(-0.6, 1.8)\n",
    "    ax[i].set_ylim(-0.6, 1.8)\n",
    "\n",
    "    ax[i].xaxis.set_major_locator(plt.MultipleLocator(0.5))\n",
    "    ax[i].yaxis.set_major_locator(plt.MultipleLocator(0.5))\n",
    "\n",
    "    ax[i].text(0.05, 0.95, titles[i],\n",
    "               ha='left', va='top', transform=ax[i].transAxes)\n",
    "\n",
    "    ax[i].set_xlabel('$g-r$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate XD \n",
    "We define compute_XD and save the result to pickle file. This cell is estimated to take a long running time (more than 20 minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_XD(n_clusters=12, rseed=0, max_iter=20, verbose=True):\n",
    "    np.random.seed(rseed)\n",
    "    clf = XDGMM(n_clusters, max_iter=max_iter, tol=1E-5, verbose=verbose)\n",
    "    clf.fit(X, Xcov)\n",
    "    return clf\n",
    "\n",
    "clf = compute_XD(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare the deconvolved colors and the stacked data\n",
    "We plot the result of noisy data after XD in scattered points (lower left), with a cluster location estimation in ellipses, in comparison with the original two data sets.   After XD resampling, the resampled data distributes less scattered than the original noisy data. It also shows a better clustered pattern than the standard star distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and sample from the underlying distribution\n",
    "np.random.seed(42)\n",
    "X_sample = clf.sample(X.shape[0])\n",
    "\n",
    "# plot the results\n",
    "fig = plt.figure(figsize=(5, 3.75))\n",
    "fig.subplots_adjust(left=0.12, right=0.95,\n",
    "                    bottom=0.1, top=0.95,\n",
    "                    wspace=0.02, hspace=0.02)\n",
    "\n",
    "# only plot 1/10 of the stars for clarity\n",
    "ax1 = fig.add_subplot(221)\n",
    "ax1.scatter(Y[::10, 2], Y[::10, 3], s=9, lw=0, c='k')\n",
    "\n",
    "ax2 = fig.add_subplot(222)\n",
    "ax2.scatter(X[::10, 2], X[::10, 3], s=9, lw=0, c='k')\n",
    "\n",
    "ax3 = fig.add_subplot(223)\n",
    "ax3.scatter(X_sample[::10, 2], X_sample[::10, 3], s=9, lw=0, c='k')\n",
    "\n",
    "ax4 = fig.add_subplot(224)\n",
    "for i in range(clf.n_components):\n",
    "    draw_ellipse(clf.mu[i, 2:4], clf.V[i, 2:4, 2:4], scales=[2],\n",
    "                 ec='k', fc='gray', alpha=0.2, ax=ax4)\n",
    "\n",
    "titles = [\"Standard Stars\", \"Single Epoch\",\n",
    "          \"Extreme Deconvolution\\n  resampling\",\n",
    "          \"Extreme Deconvolution\\n  cluster locations\"]\n",
    "ax = [ax1, ax2, ax3, ax4]\n",
    "\n",
    "for i in range(4):\n",
    "    ax[i].set_xlim(-0.6, 1.8)\n",
    "    ax[i].set_ylim(-0.6, 1.8)\n",
    "\n",
    "    ax[i].xaxis.set_major_locator(plt.MultipleLocator(0.5))\n",
    "    ax[i].yaxis.set_major_locator(plt.MultipleLocator(0.5))\n",
    "\n",
    "    ax[i].text(0.05, 0.95, titles[i],\n",
    "               ha='left', va='top', transform=ax[i].transAxes)\n",
    "\n",
    "    if i in (0, 1):\n",
    "        ax[i].xaxis.set_major_formatter(plt.NullFormatter())\n",
    "    else:\n",
    "        ax[i].set_xlabel('$g-r$')\n",
    "\n",
    "    if i in (1, 3):\n",
    "        ax[i].yaxis.set_major_formatter(plt.NullFormatter())\n",
    "    else:\n",
    "        ax[i].set_ylabel('$r-i$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Width of the stellar locus\n",
    "We plot the widths of standard stars, single epoch and XD resampled result in one graph. On the x-axis shows the width of locus, also called w color, defined as \n",
    "$w = -0.227g + 0.792r - 0.567i + 0.05$.   $\\sigma_{G}$ of the Gaussian distribution fit is the smallest in XD resampled result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astroML.stats import sigmaG\n",
    "\n",
    "# Second figure: the width of the locus\n",
    "fig = plt.figure(figsize=(5, 3.75))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "labels = ['single epoch', 'standard stars', 'XD resampled']\n",
    "linestyles = ['solid', 'dashed', 'dotted']\n",
    "for data, label, ls in zip((X, Y, X_sample), labels, linestyles):\n",
    "    g = data[:, 0]\n",
    "    gr = data[:, 2]\n",
    "    ri = data[:, 3]\n",
    "\n",
    "    r = g - gr\n",
    "    i = r - ri\n",
    "\n",
    "    mask = (gr > 0.3) & (gr < 1.0)\n",
    "    g = g[mask]\n",
    "    r = r[mask]\n",
    "    i = i[mask]\n",
    "\n",
    "    w = -0.227 * g + 0.792 * r - 0.567 * i + 0.05\n",
    "\n",
    "    sigma = sigmaG(w)\n",
    "\n",
    "    ax.hist(w, bins=np.linspace(-0.08, 0.08, 100), linestyle=ls,\n",
    "            histtype='step', label=label + '\\n\\t' + r'$\\sigma_G=%.3f$' % sigma,\n",
    "            density=True)\n",
    "\n",
    "ax.legend(loc=2)\n",
    "ax.text(0.95, 0.95, '$w = -0.227g + 0.792r$\\n$ - 0.567i + 0.05$',\n",
    "        transform=ax.transAxes, ha='right', va='top')\n",
    "\n",
    "ax.set_xlim(-0.07, 0.07)\n",
    "ax.set_ylim(0, 55)\n",
    "\n",
    "ax.set_xlabel('$w$')\n",
    "ax.set_ylabel('$N(w)$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### University of Washington: Machine Learning and Statistics \n",
    "\n",
    "# Lecture 5:  Gaussian Processes\n",
    "\n",
    "Andrew Connolly and Stephen Portillo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Resources for this notebook include:\n",
    "- [Textbook](https://press.princeton.edu/books/hardcover/9780691198309/statistics-data-mining-and-machine-learning-in-astronomy) Chapter 8. \n",
    "- [astroML website](https://www.astroml.org/index.html)\n",
    "\n",
    "This notebook is developed based on material from A. Connolly, Z. Ivezic, M. Juric, S. Portillo, G. Richards, B. Sipocz, J. VanderPlas, D. Hogg, Killian Weinberger and many others.\n",
    "\n",
    "The notebook and assoociated material are available from [github](https://github.com/uw-astro/astr-598a-win22).\n",
    "\n",
    "Make sure you are using the latest version of astroML, George and Statsmodels\n",
    "> pip install --pre -U astroml\n",
    "\n",
    "> pip install george\n",
    "\n",
    "> pip install statsmodels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc'></a>\n",
    "\n",
    "## This notebook includes:\n",
    "\n",
    "[Gaussian Processes: physical intuition](#physical) \n",
    "\n",
    "[Gaussian Processes: mathematically](#math)\n",
    "\n",
    "[Gaussian Processes: kernels](#kernels)\n",
    " \n",
    "[Prediction with Gaussian Processes](#prediction)\n",
    "\n",
    "[Building more complex models and kernels](#complex) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Gaussian Processes\n",
    "\n",
    "##### The bible for Gaussian Processes is [\"Gaussian Processes for Machine Learning\"](http://www.gaussianprocess.org/gpml/chapters/) by Carl Rasmussen and Christopher Williams\n",
    "\n",
    "Largely used in regression problems. Some of the major characteristics of GPs are\n",
    "- interpretive\n",
    "- provide uncertainty prediction\n",
    "- slow (some advances for 1 and 2D data see [Celerite2](https://github.com/exoplanet-dev/celerite2))\n",
    "\n",
    "Our goal is to predict $f(x)$ from the data. We can assume that  $f(x)$  is a random variable following a Normal distribution with $N(\\mu(x), \\sigma^2(x))$, where $\\mu(x)$ is the *predictive mean* and $\\sigma^2(x)$ is the *predictive variance*\n",
    "\n",
    "To do this in a data driven manner relies on knowning (or learing) the kernel or correlation between data points. \n",
    "\n",
    "#### Gaussian Processes: Physical Intuition <a id='physical'></a>\n",
    "[Go to top](#toc)\n",
    "\n",
    "In our previous approaches for regression we wanted to find the probability of $P(y'|x', D, \\theta)$ with $x'$ the point we want to evaluate, $\\theta$ the parameters of the model, $D$ the training data. What if we wanted to find $P(y'|x',D)$ without relying on a model. How? \n",
    "\n",
    "We could marginalize over the model parameters. From Bayes,\n",
    "\n",
    "$$P(y'|x',D) = \\int_\\theta P(y'|x',\\theta) P(\\theta|D) d\\theta$$\n",
    "\n",
    "$P(y'|x',\\theta)$ can be expressed as a Normal distribution\n",
    "\n",
    "$P(\\theta|D)=P(D|\\theta)P(\\theta)$ which can be expressed as Normal distributions \n",
    "\n",
    "\n",
    "So $P(y'|x',D)$ is a Normal distribution (e.g. see conjugate priors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume $P(y'|x',D) \\sim N(\\mu, \\Sigma)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img width=50%,  src=\"figures/gaussian.png\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the key points to GPs is that the input and predicted values will be correlated\n",
    "\n",
    "If $y_{train}$ is large then $y'$ will be large\n",
    "\n",
    "If $y_{train}$ is small then $y'$ will be small\n",
    "\n",
    "How closely they track one another depends on the values in $\\Sigma$ (and in particular the off diagonal elements)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise: imagine 3 points along a light curve. One $y_0$ at time $t=0$, one with $y_1$ at time $t=1$ and another with $y_2$ at $t=100$. Write down what the correlation should be between these points in terms of the covariance matrix.\n",
    "\n",
    "$$\\mathbf{K} = \\left(\\begin{array}{cccc}\n",
    " - & -  & -   \\\\\n",
    " - & -  & -   \\\\\n",
    " - & -  & -   \\\\\n",
    "\\end{array}\\right)$$\n",
    "\n",
    "This is a positive definite correlation/covariance matrix -> a kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian Processes: Mathematically  <a id='math'></a>\n",
    "[Go to top](#toc)\n",
    "\n",
    "Data $x,y,dy$ are represented as a Gaussian Random Process. \n",
    "\n",
    "A random process is a function $f()$ where\n",
    "- at location x, f(x) is a random variable\n",
    "- $f(x_i)$ and $f(x_j)$ are correlated\n",
    "- correlation strength is given by a kernel \n",
    "\n",
    "Random variable: variable drawn from a Normal with mean $\\mu$ and variance $\\sigma^2$\n",
    "\n",
    "Random process (infinite random variables): characterized by the mean *function* $\\mu(x)$ and covariance K=$\\sigma^2(x_i, x_j)$. With $K$ the covariance or kernel function. At this point most texts wax lyrical about the idea of infinite random variables.\n",
    "\n",
    "Therefore for a single point $x'$, $f(x')$ follows a Normal distribution. For a set of points $(x_0, x_1, \\cdots)$, $f() = (f(x_0), f(x_1), \\cdots)$ follows a multivariate Normal distribution.\n",
    "\n",
    "$$\\mathbf{f()} \\sim N\\left(\\left(\\begin{array}{c}\n",
    "\\mu(x_0) \\\\\n",
    "\\mu(x_1) \\\\\n",
    ". \\\\\n",
    ". \\\\\n",
    "\\mu(x_0) \\\\\n",
    "\\end{array}\\right), \n",
    "\\sigma^2 \\left(\\begin{array}{ccc}\n",
    "K(x_0,x_0) & K(x_0,x_1) & \\cdots  \\\\\n",
    "K(x_1,x_0) & K(x_1,x_1) & \\cdots  \\\\\n",
    ". & . & \\cdots \\\\\n",
    ". & . & \\cdots \\\\\n",
    "\\end{array}\\right)\\right)$$\n",
    "\n",
    "We can now infer the most likely $\\mu(x)$, $K(x_i, x_j)$, $\\sigma$ that will produce the observed vales (i.e. the observed y's are a random draw from this multivariate Normal distribution).\n",
    "\n",
    "We learn $\\mu(x)$, $K(x_i, x_j)$, $\\sigma$ with maximum likelihood estimation and then can predict $y'$ given $x'$. For most implementations we **learn the hyperparameters** associated with the kernel\n",
    "\n",
    "K is symmetric and postive definite and often assumed to be stationary (only depends on distance)\n",
    "\n",
    "In multiple dimensions (M),\n",
    "$K(x_i, x_j) = \\prod_{k=1}^{M} K(x_i^k, x_j^k)$\n",
    "which means that the kernel is separable - we multiple 1 D kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Common Kernels   <a id='kernels'></a>\n",
    "[Go to top](#toc)\n",
    "\n",
    "<center><img width=50%,  src=\"figures/Squared-exponential-Matern-kernel.png\"></center>\n",
    "Credit:Florian M. Heckmeier\n",
    "\n",
    "\n",
    "When the bandwith is large, predicted values at large distances are highly correlated. When the bandwith is small, predicted values at large distances are weakly correlated.  Many kernels are designed so that for large distances the predicted value tends to the mean (or zero).\n",
    "\n",
    "As we learn the hyper parameters for the kernel, sorting by bandwidth (large to small) enables us to determine which features in a distribution are important (i.e. the ones with a small bandwidth) - relates to dimensionality reduction.\n",
    "\n",
    "\n",
    "##### squared exponential\n",
    "\n",
    "$$k(x_i, x_j) = \\exp(-\\theta(x_i - x_j)^2)$$\n",
    "\n",
    "##### matern\n",
    "\n",
    "$$k(x_i, x_j) =  \\frac{1}{\\Gamma(\\nu)2^{\\nu-1}}\\Bigg(\n",
    "\\frac{\\sqrt{2\\nu}}{l} d(x_i , x_j )\n",
    "\\Bigg)^\\nu K_\\nu\\Bigg(\n",
    "\\frac{\\sqrt{2\\nu}}{l} d(x_i , x_j )\\Bigg)$$\n",
    "\n",
    "where $d(\\cdot,\\cdot)$ is the Euclidean distance,\n",
    "$K_{\\nu}(\\cdot)$ is a modified Bessel function and\n",
    "$\\Gamma(\\cdot)$ is the gamma function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction with a Gaussian Process    <a id='prediction'></a>\n",
    "[Go to top](#toc)\n",
    "\n",
    "Given the training data we have a large multivariate Normal distribution\n",
    "\n",
    "$P(y_0, y_1,..y_N|x_0, x_1,..x_N) \\sim N()$\n",
    "\n",
    "we want to predict $y'$ which is not a deterministic value but a random variable following a Normal distribution. We want,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\left(\\begin{array}{c}\n",
    "y \\\\\n",
    "y'\n",
    "\\end{array}\\right)\n",
    "\\sim N \\left( \\mu, \\sigma^2 \\left(\\begin{array}{cc}\n",
    "K & k'  \\\\\n",
    "k'^T & k'' \n",
    "\\end{array}\\right)\\right) $$\n",
    "\n",
    "where $K$ is the correlation matrix from the data, $k'$ is a correlation vector between the test (evaluated point) and training data, and  $k''$ variance of the test point (can be normalized to unity).\n",
    "\n",
    "$$k' =  \\left(\\begin{array}{c}\n",
    "K(x',x_0)  \\\\\n",
    "K(x',x_1) \\\\\n",
    "K(x',x_2) \\\\\n",
    ". \\\\\n",
    ". \n",
    "\\end{array}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is essentially saying that we want  $P(y' | y_0, y_1,..y_N,x_0, x_1,..x_N, x')$ which is y' given x', conditional on $y_0, y_1,..y_N,x_0, x_1,..x_N$\n",
    "\n",
    "We can derive $P(y'|y)$ which describes how $y'$ is distributed given the data y. This conditional distribution is also a Normal distribution with , i.e., $P(y'|y) \\sim N(\\mu', \\Sigma')$ with,\n",
    "\n",
    "$\\mu' = \\mu + k'^T K^{-1} (y - I \\mu)$\n",
    "\n",
    "$\\Sigma' = \\sigma^2(1- k'^T K^{-1} k')$\n",
    "\n",
    "We use the mean $\\mu'$ as the expected prediction value and $\\Sigma'$ as the prediction uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "import sklearn.gaussian_process.kernels as k\n",
    "\n",
    "# Create the noisy data\n",
    "Nrandom = 20\n",
    "\n",
    "def f(x):\n",
    "    '''function to model'''\n",
    "    h = 6.*np.exp(-0.5 * ((x - 4.) / 0.25) ** 2) + x * np.sin(x) + 10.\n",
    "    return h\n",
    "\n",
    "# points to evaluate the model\n",
    "x_model = np.linspace(0,9,100)\n",
    "y_model = f(x_model)\n",
    "\n",
    "#select a subset of these points\n",
    "x = x_model[np.sort(np.random.randint (0, 100, Nrandom))]\n",
    "y_true = f(x)\n",
    "dy = 0.2 + 1.9 * np.random.random(y_true.shape)\n",
    "noise = np.random.normal(0, dy*np.ones(Nrandom))\n",
    "y = y_true + noise\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x_model, y_model, 'r:')\n",
    "plt.errorbar(x, y, dy, fmt='r.', markersize=10, label=u'Observations')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$f(x)$')\n",
    "plt.ylim(0, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using scikit-learn we can apply a GP to these data (initially noise free). First we define the Kernel (see sklearn.gaussian_process.kernels). For this can use a radial basis function (equivalent to the squared exponential) and we learn the hyperparameters from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a kernel - squared exponential, normalized by variance, \n",
    "kernel = np.var(y)*k.RBF(length_scale=10.0)\n",
    "gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=90)\n",
    "gp.fit(np.atleast_2d(x).T, y_true)\n",
    "print(\"Optimized hyper parameters: \", gp.kernel_)\n",
    "\n",
    "# Make the prediction on the meshed x-axis (ask for MSE as well)\n",
    "y_pred, sigma = gp.predict(x_model[:,None], return_std=True)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x_model, y_model, 'r:')\n",
    "plt.scatter(x, y_true, c='r', s=30)\n",
    "#plt.errorbar(x, y, dy, fmt='r.', markersize=10, label=u'Observations')\n",
    "plt.plot(x_model, y_pred, 'b-', label=u'Prediction')\n",
    "\n",
    "plt.fill_between(\n",
    "    x_model,\n",
    "    y_pred - 1.96 * sigma,\n",
    "    y_pred + 1.96 * sigma,\n",
    "    alpha=0.3,\n",
    "    label=r\"95% confidence interval\",\n",
    ")\n",
    "\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$f(x)$')\n",
    "plt.ylim(0, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will repeat the exercise including the noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = np.var(y)*k.RBF(length_scale=20.0)\n",
    "\n",
    "gp = GaussianProcessRegressor(\n",
    "    kernel=kernel, alpha=0.9**2, n_restarts_optimizer=90\n",
    ")\n",
    "gp.fit(np.atleast_2d(x).T, y)\n",
    "print(\"Optimized hyper parameters: \", gp.kernel_)\n",
    "\n",
    "# Make the prediction on the meshed x-axis (ask for MSE as well)\n",
    "y_pred, sigma = gp.predict(x_model[:,None], return_std=True)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x_model, y_model, 'r:')\n",
    "plt.errorbar(x, y, dy, fmt='r.', markersize=10, label=u'Observations')\n",
    "plt.plot(x_model, y_pred, 'b-', label=u'Prediction')\n",
    "\n",
    "plt.fill_between(\n",
    "    x_model,\n",
    "    y_pred - 1.96 * sigma,\n",
    "    y_pred + 1.96 * sigma,\n",
    "    alpha=0.3,\n",
    "    label=r\"95% confidence interval\",\n",
    ")\n",
    "\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$f(x)$')\n",
    "plt.ylim(0, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see what is happening with the fitting we can plot the prior and posterior of a GaussianProcessRegressor  (see [scikit-learn](https://scikit-learn.org/stable/auto_examples/gaussian_process/plot_gpr_prior_posterior.html#sphx-glr-auto-examples-gaussian-process-plot-gpr-prior-posterior-py))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gpr_samples(gpr_model, n_samples, ax):\n",
    "    \"\"\"Plot samples drawn from the Gaussian process model.\n",
    "\n",
    "    If the Gaussian process model is not trained then the drawn samples are\n",
    "    drawn from the prior distribution. Otherwise, the samples are drawn from\n",
    "    the posterior distribution. Be aware that a sample here corresponds to a\n",
    "    function.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    gpr_model : `GaussianProcessRegressor`\n",
    "        A :class:`~sklearn.gaussian_process.GaussianProcessRegressor` model.\n",
    "    n_samples : int\n",
    "        The number of samples to draw from the Gaussian process distribution.\n",
    "    ax : matplotlib axis\n",
    "        The matplotlib axis where to plot the samples.\n",
    "    \"\"\"\n",
    "    x = np.linspace(0, 9, 100)\n",
    "    X = x.reshape(-1, 1)\n",
    "\n",
    "    y_mean, y_std = gpr_model.predict(X, return_std=True)\n",
    "    y_samples = gpr_model.sample_y(X, n_samples)\n",
    "\n",
    "    for idx, single_prior in enumerate(y_samples.T):\n",
    "        ax.plot(\n",
    "            x,\n",
    "            single_prior,\n",
    "            linestyle=\"--\",\n",
    "            alpha=0.7,\n",
    "            label=f\"Sampled function #{idx + 1}\",\n",
    "        )\n",
    "    ax.plot(x, y_mean, color=\"black\", label=\"Mean\")\n",
    "    ax.fill_between(\n",
    "        x,\n",
    "        y_mean - y_std,\n",
    "        y_mean + y_std,\n",
    "        alpha=0.1,\n",
    "        color=\"black\",\n",
    "        label=r\"$\\pm$ 1 std. dev.\",\n",
    "    )\n",
    "    ax.set_xlabel(\"x\")\n",
    "    ax.set_ylabel(\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the kernel and the GaussianProcessRegressor\n",
    "kernel = np.var(y)*k.RBF(length_scale=20.0)\n",
    "\n",
    "gp = GaussianProcessRegressor(\n",
    "    kernel=kernel, alpha=0.9**2, n_restarts_optimizer=90\n",
    ")\n",
    "\n",
    "fig, axs = plt.subplots(nrows=2, sharex=True, sharey=True, figsize=(4, 8))\n",
    "\n",
    "# plot the prior\n",
    "n_samples=10\n",
    "plot_gpr_samples(gp, n_samples=n_samples, ax=axs[0])\n",
    "axs[0].set_title(\"Samples from prior distribution\")\n",
    "\n",
    "# plot the posterior\n",
    "gp.fit(np.atleast_2d(x).T, y)\n",
    "plot_gpr_samples(gp, n_samples=n_samples, ax=axs[1])\n",
    "axs[1].errorbar(x, y, dy, fmt='r.', markersize=10, label=u'Observations')\n",
    "axs[1].legend(bbox_to_anchor=(1.05, 1.5), loc=\"upper left\")\n",
    "axs[1].set_title(\"Samples from posterior distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we will use [George](http://dfm.io/george/dev/) which is \"designed to be used alongside your favorite non-linear optimization or posterior inference library for the best results.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import george\n",
    "from george import kernels\n",
    "\n",
    "kernel = np.var(y) * kernels.ExpSquaredKernel(10.)\n",
    "gp = george.GP(kernel)\n",
    "gp.compute(x, dy)\n",
    "\n",
    "pred, pred_var = gp.predict(y, x_model, return_var=True)\n",
    "\n",
    "plt.fill_between(x_model, pred - np.sqrt(pred_var), pred + np.sqrt(pred_var),\n",
    "                color=\"k\", alpha=0.2)\n",
    "plt.plot(x_model, pred, \"k\", lw=1.5, alpha=0.5)\n",
    "plt.errorbar(x, y, yerr=dy, fmt=\".k\", capsize=0)\n",
    "plt.plot(x_model, y_model, \"--g\")\n",
    "\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.optimize as op\n",
    "\n",
    "# Define the objective function (negative log-likelihood in this case).\n",
    "def nll(p):\n",
    "    gp.set_parameter_vector(p)\n",
    "    ll = gp.log_likelihood(y, quiet=True)\n",
    "    return -ll if np.isfinite(ll) else 1e25\n",
    "\n",
    "# And the gradient of the objective function.\n",
    "def grad_nll(p):\n",
    "    gp.set_parameter_vector(p)\n",
    "    return -gp.grad_log_likelihood(y, quiet=True)\n",
    "\n",
    "# You need to compute the GP once before starting the optimization.\n",
    "gp.compute(x, dy)\n",
    "print (\"Parameters prior to optimization: \", gp.get_parameter_vector())\n",
    "\n",
    "# Print the initial ln-likelihood.\n",
    "print(\"Log-likelihood prior to optimization: \", gp.log_likelihood(y))\n",
    "\n",
    "# Run the optimization routine.\n",
    "p0 = gp.get_parameter_vector()\n",
    "results = op.minimize(nll, p0, jac=grad_nll, method=\"L-BFGS-B\")\n",
    "print (\"Parameters after  optimization: \", results.x)\n",
    "\n",
    "# Update the kernel and print the final log-likelihood.\n",
    "gp.set_parameter_vector(results.x)\n",
    "print(\"Log-likelihood after to optimization: \", gp.log_likelihood(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred, pred_var = gp.predict(y, x_model, return_var=True)\n",
    "\n",
    "plt.fill_between(x_model, pred - np.sqrt(pred_var), pred + np.sqrt(pred_var),\n",
    "                color=\"k\", alpha=0.2)\n",
    "plt.plot(x_model, pred, \"k\", lw=1.5, alpha=0.5)\n",
    "plt.errorbar(x, y, yerr=dy, fmt=\".k\", capsize=0)\n",
    "plt.plot(x_model, y_model, \"--g\")\n",
    "\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0q9zlSEuBtGi"
   },
   "source": [
    "### Building more complex models and kernels <a id='complex'></a>\n",
    "[Go to top](#toc)\n",
    "\n",
    "We can build complex kernels based on the properties of the data. Here we take a classic case from the original Rasmussen and Williams book; modeling the monthly average atmospheric CO2 concentrations (in parts per million by volume (ppmv)) collected at the Mauna Loa Observatory (1958 to 1997). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.datasets import co2\n",
    "\n",
    "data = co2.load_pandas().data\n",
    "t = 2000 + (np.array(data.index.to_julian_date()) - 2451545.0) / 365.25\n",
    "y = np.array(data.co2)\n",
    "m = np.isfinite(t) & np.isfinite(y) & (t < 1996)\n",
    "t, y = t[m][::4], y[m][::4]\n",
    "\n",
    "plt.plot(t, y, \".k\")\n",
    "plt.xlim(t.min(), t.max())\n",
    "plt.xlabel(\"year\")\n",
    "plt.ylabel(\"CO$_2$ in ppm\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: we want to model  the CO2 concentration as a function of time. To do this we need to account for \n",
    "- a long term increase in the CO2 concentrate. A RBF kernel (RBF: scikit-learn, ExpSquaredKernel: George) can account for this sort of trend\n",
    "- a periodic or seasonal variation. A periodic kernel (ExpSineSquared: scikit-learn,  ExpSine2Kernel: George)  can capture this if we set the period\n",
    "- medium scale irregularities. These can be explained by a rational quadratic kernel (RationalQuadratic: scikit-learn,  RationalQuadraticKernel: George) \n",
    "- a noise term. This can be captured by a whitening term (WhiteKernel: scikit-learn,  ExpSquaredKernel: George) \n",
    "\n",
    "Build a kernel by summing the individual kernel components (e.g. kernel = k.RBF() + k.WhiteKernel()) and show how well you can model the data. You will need to think about the parameters that you should set for the kernels. Start with just a one or two component kernel before advancing to the full model \n",
    "\n",
    "Use scikit-learn, or George, or any other GP package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
